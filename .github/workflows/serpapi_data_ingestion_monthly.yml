name: Monthly Job Ingestion

on:
  # 1. Schedule: Run at 00:00 UTC on the 5st of every month
  schedule:
    - cron: '0 0 5 * *'
  # 2. Manual Trigger: Allows you to click "Run workflow" in GitHub UI to test it
  workflow_dispatch:

permissions:
  contents: write # Required to push the updated dvc.lock file back to the repo

jobs:
  run-pipeline:
    runs-on: ubuntu-latest
    
    steps:
      # --- Setup Phase ---
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      # --- DVC Configuration (CRITICAL) ---
      # This step authenticates DVC to read/write from your Storage
      # NOTE: This example assumes DagsHub storage because GDrive in CI is complex.
      # If using GDrive, you need a Service Account (see notes below).
      - name: Configure DVC Remote (DagsHub)
        env:
          DAGSHUB_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc remote modify origin --local auth basic
          dvc remote modify origin --local user $DAGSHUB_USERNAME
          dvc remote modify origin --local password $DAGSHUB_TOKEN

      # --- Execution Phase ---
      - name: Run DVC Pipeline (Repro)
        env:
          # Inject Secrets into the Environment so python-dotenv finds them
          SERPAPI_KEY: ${{ secrets.SERPAPI_KEY }}
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.DAGSHUB_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          # Force re-execution of ingest stage even if DVC thinks it's up to date
          dvc repro --force ingest
          
          # Optional: Run processing immediately after
          # dvc repro process

      # --- Storage Phase ---
      - name: Push Data to Storage
        run: dvc push -r origin

      # --- Git Sync Phase ---
      - name: Commit and Push dvc.lock
        run: |
          git config --global user.name 'GitHub Actions Bot'
          git config --global user.email 'actions@github.com'
          
          # Stage only the lock file (proof of new data)
          git add dvc.lock
          
          # Commit and Push if there are changes
          # The '|| echo' prevents failure if there's nothing to commit
          git diff-index --quiet HEAD || git commit -m "ðŸ¤– Cron: Monthly Data Ingestion for $(date +'%Y-%m')"
          git push




          name: Monthly Data Ingestion

on:
  schedule:
    # Run at 00:00 on the 5th of every month
    - cron: '0 0 5 * *'
  # Allow manual trigger for testing
  workflow_dispatch:

jobs:
  ingest-and-save:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      # 1. Setup Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # 2. Install Dependencies
      - name: Install Libraries
        run: pip install -r requirements.txt

      # 3. Pull Config Data (Fixes the "Invisible File" issue)
      - name: Pull Constants from DVC
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          dvc remote modify myremote access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          dvc remote modify myremote secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          dvc pull data/constants/ -v

      # 4. Run the Ingestion Script
      - name: Run SerpAPI Ingest
        env:
          SERPAPI_API_KEY: ${{ secrets.SERPAPI_API_KEY }}
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}
          MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}
        # We use a custom "Update Params" logic or pass date via env if needed
        run: |
           python src/update_params.py 
           python data_ingestion/jd_ingestion/serp_api/serpapi_ingest.py

      # 5. Save Results to Cloud (The Fix for Amnesia)
      - name: Push New Data to S3 & Git
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          # Add the new JSON files to DVC
          dvc add data/raw/serpapi/
          
          # Push the actual heavy data to S3
          dvc push
          
          # Commit the new .dvc pointer file to GitHub
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add data/raw/serpapi.dvc params.yaml
          git commit -m "ðŸ¤– Monthly Data Ingest: $(date +'%Y-%m-%d')"
          git push