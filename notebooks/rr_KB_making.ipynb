{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQZbxWdEMJWS"
   },
   "outputs": [],
   "source": [
    "p1=\"/content/drive/Othercomputers/My laptop/Desktop/ResumeRecommenderMLops/data/constants/KB/detailed_job_descriptions.json\"\n",
    "p2=\"/content/drive/Othercomputers/My laptop/Desktop/ResumeRecommenderMLops/data/processed/serpapi/2026-01.csv\"\n",
    "p3=\"/content/drive/Othercomputers/My laptop/Desktop/ResumeRecommenderMLops/data/constants/wiki_llm_training_resume_project.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7OaYxneMKTG"
   },
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "You are a Knowledge Graph Extraction Engine for a Career Advisory System.\n",
    "\n",
    "Your task is to extract structured knowledge that explains:\n",
    "1. WHAT a job role requires.\n",
    "2. WHY those requirements exist (conceptual grounding).\n",
    "3. HOW skills transfer across roles (gap and similarity reasoning).\n",
    "\n",
    "This knowledge graph will be used to explain to candidates:\n",
    "- Why a job is their best match\n",
    "- Why other roles are second or third best\n",
    "- What skills or concepts they should learn next\n",
    "\n",
    "You must strictly follow the ontology, rules, and output format below.\n",
    "If something does not fit the rules, DO NOT output it.\n",
    "\n",
    "--------------------------------------------------\n",
    "ONTOLOGY CONTRACT (STRICT)\n",
    "--------------------------------------------------\n",
    "\n",
    "ALLOWED NODE TYPES:\n",
    "JobPosting\n",
    "JobRole\n",
    "Skill\n",
    "ProgrammingLanguage\n",
    "Framework\n",
    "Tool\n",
    "Platform\n",
    "CloudService\n",
    "Database\n",
    "Concept\n",
    "Company\n",
    "Location\n",
    "\n",
    "--------------------------------------------------\n",
    "ALLOWED RELATIONS (SEMANTIC)\n",
    "--------------------------------------------------\n",
    "\n",
    "-- Job Structure --\n",
    "POSTED_BY          (JobPosting -> Company)\n",
    "LOCATED_IN         (Company -> Location)\n",
    "IS_FOR_ROLE        (JobPosting -> JobRole)\n",
    "RELATED_ROLE       (JobRole -> JobRole)\n",
    "\n",
    "-- Role Requirements (EXPLANATION-ORIENTED) --\n",
    "REQUIRES_SKILL     (JobRole -> Skill)\n",
    "REQUIRES_TOOL      (JobRole -> Tool | Framework | Database | Platform | CloudService)\n",
    "REQUIRES_LANGUAGE  (JobRole -> ProgrammingLanguage)\n",
    "REQUIRES_CONCEPT   (JobRole -> Concept)\n",
    "\n",
    "-- Technical Reasoning (WHY / HOW) --\n",
    "USES_LANGUAGE       (Framework | Tool -> ProgrammingLanguage)\n",
    "IMPLEMENTS_CONCEPT  (Tool | Framework | ProgrammingLanguage -> Concept)\n",
    "IS_SIMILAR_TO       (Tool | Framework | Database | CloudService -> same type)\n",
    "BUILT_WITH          (Tool -> ProgrammingLanguage)\n",
    "CREATED_BY          (ProgrammingLanguage | Framework | Tool -> Company)\n",
    "\n",
    "--------------------------------------------------\n",
    "ALLOWED TRIPLE PATTERNS (MANDATORY)\n",
    "--------------------------------------------------\n",
    "\n",
    "You may emit a triplet ONLY if it matches one of the following exact patterns:\n",
    "\n",
    "-- Job Posting Context --\n",
    "(JobPosting) IS_FOR_ROLE (JobRole)\n",
    "(JobPosting) POSTED_BY (Company)\n",
    "(Company) LOCATED_IN (Location)\n",
    "\n",
    "-- Role Definition Context --\n",
    "(JobRole) REQUIRES_SKILL (Skill)\n",
    "(JobRole) REQUIRES_TOOL (Tool | Framework | Database | Platform | CloudService)\n",
    "(JobRole) REQUIRES_LANGUAGE (ProgrammingLanguage)\n",
    "(JobRole) REQUIRES_CONCEPT (Concept)\n",
    "(JobRole) RELATED_ROLE (JobRole)\n",
    "\n",
    "-- Technical / Knowledge Context --\n",
    "(Framework | Tool | ProgrammingLanguage) IMPLEMENTS_CONCEPT (Concept)\n",
    "(Framework | Tool) USES_LANGUAGE (ProgrammingLanguage)\n",
    "(Tool | Framework | Database) IS_SIMILAR_TO (same type)\n",
    "(Tool) BUILT_WITH (ProgrammingLanguage)\n",
    "(ProgrammingLanguage | Framework | Tool) CREATED_BY (Company)\n",
    "\n",
    "If a candidate triplet does NOT match these patterns â†’ DISCARD IT.\n",
    "\n",
    "--------------------------------------------------\n",
    "FORBIDDEN OUTPUT (NEVER EMIT)\n",
    "--------------------------------------------------\n",
    "\n",
    "Do NOT output:\n",
    "- Responsibilities or actions\n",
    "- Sentences or long phrases as nodes\n",
    "- Soft skills (communication, leadership, teamwork)\n",
    "- Abstract fillers (systems, techniques, solutions)\n",
    "- Version numbers or editions\n",
    "- Salary, experience ranges, dates\n",
    "- Statistics or percentages\n",
    "\n",
    "--------------------------------------------------\n",
    "SOURCE-AWARE RULES\n",
    "--------------------------------------------------\n",
    "\n",
    "IF INPUT SOURCE IS JOB DESCRIPTION (CSV):\n",
    "- Treat the record as a JobPosting.\n",
    "- Always map JobPosting to a normalized JobRole using IS_FOR_ROLE.\n",
    "- Extract only explicitly mentioned skills, tools, frameworks, languages, or databases.\n",
    "- Ignore benefits, culture, or generic HR text.\n",
    "\n",
    "IF INPUT SOURCE IS ROLE DEFINITION (JSON):\n",
    "- Subject is the JobRole.\n",
    "- Extract core technical expectations as REQUIRES_* relations.\n",
    "- Extract adjacent or alternative roles using RELATED_ROLE.\n",
    "\n",
    "IF INPUT SOURCE IS WIKI ARTICLE:\n",
    "- DO NOT emit JobPosting or JobRole.\n",
    "- Focus only on ProgrammingLanguage, Framework, Tool, Concept.\n",
    "- Emit Company ONLY if it is the creator of the technology (CREATED_BY).\n",
    "- Emit EXACTLY ONE definition object for the primary entity described.\n",
    "\n",
    "--------------------------------------------------\n",
    "WIKI-SPECIFIC: DEFINITION EXTRACTION (MANDATORY)\n",
    "--------------------------------------------------\n",
    "\n",
    "If and ONLY if the input source is a Wiki article, return a definition object\n",
    "focused on UTILITY and EXPLANATION.\n",
    "\n",
    "Definition schema:\n",
    "- definition: ONE sentence explaining what it is and why it is useful.\n",
    "- summary: 2 sentences explaining real-world usage.\n",
    "- key_characteristics: 3â€“5 short technical bullet points.\n",
    "- source: provenance string (e.g. \"wikipedia\").\n",
    "\n",
    "Do NOT include history, dates, or trivia.\n",
    "\n",
    "--------------------------------------------------\n",
    "ENTITY NORMALIZATION RULES\n",
    "--------------------------------------------------\n",
    "\n",
    "- Canonical names only (e.g., \"JS\" -> \"JavaScript\").\n",
    "- Singular form only.\n",
    "- Title Case for entity names.\n",
    "- Disambiguate ambiguous terms using context.\n",
    "- Prefer CloudService over Platform for AWS, Azure, GCP.\n",
    "\n",
    "--------------------------------------------------\n",
    "OUTPUT FORMAT (STRICT JSON ONLY)\n",
    "--------------------------------------------------\n",
    "\n",
    "Return ONLY a valid JSON object. No markdown. No explanations.\n",
    "\n",
    "{\n",
    "  \"triplets\": [\n",
    "    {\n",
    "      \"subject\": \"string\",\n",
    "      \"subject_type\": \"string (from ALLOWED NODE TYPES)\",\n",
    "      \"relation\": \"string (from ALLOWED RELATIONS)\",\n",
    "      \"object\": \"string\",\n",
    "      \"object_type\": \"string (from ALLOWED NODE TYPES)\"\n",
    "    }\n",
    "  ],\n",
    "  \"definitions\": [\n",
    "    {\n",
    "      \"name\": \"string\",\n",
    "      \"node_type\": \"string (from ALLOWED NODE TYPES)\",\n",
    "      \"definition\": \"string\",\n",
    "      \"summary\": \"string\",\n",
    "      \"key_characteristics\": [\"string\"],\n",
    "      \"source\": \"string\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_RAOKDcMKat"
   },
   "outputs": [],
   "source": [
    "!pip install -q google-genai pandas tqdm groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h48BM6_Qe-WX"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "api_key = userdata.get('temp_gemini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCYWsxMJM6VE"
   },
   "source": [
    "MAKING TRIPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRiaOEHpMKbg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    # API Configuration\n",
    "    API_KEY = api_key  # <--- PASTE YOUR KEY HERE\n",
    "    MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "    # File Paths\n",
    "    PATH_ROLES = \"/content/drive/Othercomputers/My laptop/Desktop/ResumeRecommenderMLops/data/constants/KB/detailed_job_descriptions.json\"\n",
    "    PATH_JOBS = \"/content/drive/Othercomputers/My laptop/Desktop/ResumeRecommenderMLops/data/processed/serpapi/2026-01.csv\"\n",
    "    PATH_WIKI = \"/content/drive/Othercomputers/My laptop/Desktop/ResumeRecommenderMLops/data/constants/wiki_llm_training_resume_project.jsonl\"\n",
    "\n",
    "    # Batching Settings\n",
    "    BATCH_SIZE_CHARS = 53000  # Character limit per batch (safe for context)\n",
    "\n",
    "    # Output\n",
    "    LOCAL_OUTPUT_FILE = \"knowledge_graph_extracted_DAY13.jsonl\"\n",
    "\n",
    "    # Backup Settings (Google Drive)\n",
    "    # Ensure this folder exists in your Drive!\n",
    "    DRIVE_BACKUP_DIR = \"/content/drive/MyDrive/rr_mlops\"\n",
    "    DRIVE_BACKUP_PATH = os.path.join(DRIVE_BACKUP_DIR, LOCAL_OUTPUT_FILE)\n",
    "\n",
    "    RESUME_FROM_BATCH= 153\n",
    "\n",
    "# ==========================================\n",
    "# 2. SYSTEM PROMPT (CONSTANTS)\n",
    "# ==========================================\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Knowledge Graph Extraction Engine for a Career Advisory System.\n",
    "\n",
    "Your task is to extract structured knowledge that enables EXPLANATION, not just storage.\n",
    "\n",
    "This knowledge graph must support:\n",
    "1. WHY a job is the best match for a candidate\n",
    "2. WHY other roles are second or third best\n",
    "3. WHAT specific skills or concepts the candidate is missing\n",
    "4. HOW skills transfer across roles via shared concepts\n",
    "\n",
    "You must STRICTLY follow the ontology, rules, and output format below.\n",
    "If something does not fit the rules, DO NOT output it.\n",
    "\n",
    "--------------------------------------------------\n",
    "ONTOLOGY CONTRACT (STRICT)\n",
    "--------------------------------------------------\n",
    "\n",
    "ALLOWED NODE TYPES:\n",
    "JobPosting\n",
    "JobRole\n",
    "Skill\n",
    "ProgrammingLanguage\n",
    "Framework\n",
    "Tool\n",
    "Platform\n",
    "CloudService\n",
    "Database\n",
    "Concept\n",
    "Company\n",
    "Location\n",
    "\n",
    "--------------------------------------------------\n",
    "CORE SEMANTIC PRINCIPLE (MANDATORY)\n",
    "--------------------------------------------------\n",
    "\n",
    "The graph MUST separate:\n",
    "- EXECUTION (Skill)\n",
    "- EXPLANATION (Concept)\n",
    "\n",
    "This separation is NON-NEGOTIABLE.\n",
    "\n",
    "--------------------------------------------------\n",
    "SKILL vs CONCEPT CLASSIFICATION (CRITICAL RULE)\n",
    "--------------------------------------------------\n",
    "\n",
    "Skill = \"Can this be directly practiced, implemented, or demonstrated?\"\n",
    "\n",
    "Concept = \"Does this explain WHY or WHAT principle something is based on?\"\n",
    "\n",
    "HARD CONSTRAINT:\n",
    "If a Skill candidate can be rewritten as\n",
    "\"Understanding of X\" or \"Knowledge of X\",\n",
    "it MUST be classified as Concept.\n",
    "\n",
    "ANCHORING RULE:\n",
    "Every Skill emitted for a JobRole SHOULD be supported\n",
    "by at least one Concept via either:\n",
    "- JobRole REQUIRES_CONCEPT\n",
    "- Tool IMPLEMENTS_CONCEPT\n",
    "\n",
    "If no Concept exists add one according to your knowledge.\n",
    "\n",
    "\n",
    "VALID Skill examples:\n",
    "- SQL Query Writing\n",
    "- REST API Development\n",
    "- Feature Engineering\n",
    "- Model Deployment\n",
    "- Prompt Engineering\n",
    "- Kubernetes Troubleshooting\n",
    "- CI/CD Pipeline Implementation\n",
    "- Data Visualization\n",
    "- Unit Testing\n",
    "- API Integration\n",
    "\n",
    "INVALID as Skill (MUST be Concept):\n",
    "- Machine Learning\n",
    "- Deep Learning\n",
    "- Object-Oriented Programming\n",
    "- System Design\n",
    "- Cloud Architecture\n",
    "- Distributed Systems\n",
    "- Microservices Architecture\n",
    "- Natural Language Processing\n",
    "- Security Best Practices\n",
    "- Data Modeling\n",
    "- Statistics\n",
    "\n",
    "RULES:\n",
    "- If a term explains WHY â†’ Concept\n",
    "- If a term explains HOW â†’ Skill\n",
    "- If uncertain â†’ Concept\n",
    "- NEVER attach vague domains as Skill\n",
    "\n",
    "--------------------------------------------------\n",
    "TOOL vs FRAMEWORK vs PLATFORM (STRICT DEFINITIONS)\n",
    "--------------------------------------------------\n",
    "Classify technical entities using these rules:\n",
    "\n",
    "1. FRAMEWORK: \"Inverts control\". It calls your code. It provides a scaffold.\n",
    "   - EXAMPLES: React, Angular, Vue, Django, Spring Boot, TensorFlow, PyTorch, Flutter.\n",
    "   - RULE: If it dictates the architecture of the app, it is a Framework.\n",
    "\n",
    "2. TOOL: A library, utility, or software used to perform a task. You call it.\n",
    "   - EXAMPLES: Pandas (Library -> Tool), NumPy (Library -> Tool), Git (CLI Tool), Docker (Container Tool), Jenkins (Build Tool), Jira (Project Tool).\n",
    "   - RULE: If you import it to do math/data manipulation, it is a Tool (Library).\n",
    "\n",
    "3. PLATFORM: An environment where software runs or is hosted.\n",
    "   - EXAMPLES: iOS, Android, Linux, Windows, GitHub (SaaS Platform), GitLab.\n",
    "\n",
    "--------------------------------------------------\n",
    "ALLOWED RELATIONS (SEMANTIC)\n",
    "--------------------------------------------------\n",
    "\n",
    "-- Job Structure --\n",
    "POSTED_BY          (JobPosting -> Company)\n",
    "LOCATED_IN         (Company -> Location)\n",
    "IS_FOR_ROLE        (JobPosting -> JobRole)\n",
    "RELATED_ROLE       (JobRole -> JobRole)\n",
    "\n",
    "-- Role Requirements (EXPLANATION-FIRST) --\n",
    "REQUIRES_SKILL     (JobRole -> Skill)\n",
    "REQUIRES_LANGUAGE  (JobRole -> ProgrammingLanguage)\n",
    "REQUIRES_TOOL      (JobRole -> Tool | Framework | Database | Platform | CloudService)\n",
    "REQUIRES_CONCEPT   (JobRole -> Concept)\n",
    "\n",
    "-- Technical Reasoning (WHY / HOW CHAINS) --\n",
    "IMPLEMENTS_CONCEPT (Tool | Framework | ProgrammingLanguage -> Concept)\n",
    "USES_LANGUAGE      (Framework | Tool -> ProgrammingLanguage)\n",
    "BUILT_WITH         (Tool -> ProgrammingLanguage)\n",
    "IS_SIMILAR_TO      (Tool | Framework | Database -> same type)\n",
    "CREATED_BY         (ProgrammingLanguage | Framework | Tool -> Company)\n",
    "\n",
    "--------------------------------------------------\n",
    "ALLOWED TRIPLE PATTERNS (MANDATORY)\n",
    "--------------------------------------------------\n",
    "\n",
    "You may emit a triplet ONLY if it matches one of these exact patterns:\n",
    "\n",
    "-- Job Posting Context --\n",
    "(JobPosting) IS_FOR_ROLE (JobRole)\n",
    "(JobPosting) POSTED_BY (Company)\n",
    "(Company) LOCATED_IN (Location)\n",
    "\n",
    "-- Role Definition Context --\n",
    "(JobRole) REQUIRES_SKILL (Skill)\n",
    "(JobRole) REQUIRES_LANGUAGE (ProgrammingLanguage)\n",
    "(JobRole) REQUIRES_TOOL (Tool | Framework | Database | Platform | CloudService)\n",
    "(JobRole) REQUIRES_CONCEPT (Concept)\n",
    "(JobRole) RELATED_ROLE (JobRole)\n",
    "\n",
    "-- Technical Knowledge Context --\n",
    "(Framework | Tool | ProgrammingLanguage) IMPLEMENTS_CONCEPT (Concept)\n",
    "(Framework | Tool) USES_LANGUAGE (ProgrammingLanguage)\n",
    "(Tool | Framework | Database) IS_SIMILAR_TO (same type)\n",
    "(Tool) BUILT_WITH (ProgrammingLanguage)\n",
    "(ProgrammingLanguage | Framework | Tool) CREATED_BY (Company)\n",
    "\n",
    "If a candidate triplet does NOT match these patterns â†’ DISCARD IT.\n",
    "\n",
    "--------------------------------------------------\n",
    "FORBIDDEN OUTPUT (NEVER EMIT)\n",
    "--------------------------------------------------\n",
    "\n",
    "Do NOT output:\n",
    "- Responsibilities or actions\n",
    "- Soft skills (communication, leadership, teamwork)\n",
    "- Abstract fillers (systems, solutions, techniques)\n",
    "- Version numbers or editions\n",
    "- Salary, experience ranges, dates\n",
    "- Metrics or percentages\n",
    "- Multi-sentence nodes\n",
    "\n",
    "--------------------------------------------------\n",
    "SOURCE-AWARE RULES\n",
    "--------------------------------------------------\n",
    "\n",
    "IF INPUT SOURCE IS JOB DESCRIPTION (CSV):\n",
    "- Treat each record as a JobPosting.\n",
    "- ALWAYS map JobPosting â†’ JobRole using IS_FOR_ROLE.\n",
    "- Extract ONLY explicitly mentioned technical requirements.\n",
    "- Do NOT infer skills not written in text.\n",
    "\n",
    "IF INPUT SOURCE IS ROLE DEFINITION (JSON):\n",
    "- Subject is the JobRole.\n",
    "- Extract REQUIRED skills, tools, languages, and concepts.\n",
    "- Extract adjacent or alternative roles using RELATED_ROLE.\n",
    "\n",
    "IF INPUT SOURCE IS WIKI ARTICLE:\n",
    "- DO NOT emit JobPosting or JobRole.\n",
    "- Focus only on ProgrammingLanguage, Framework, Tool, Concept.\n",
    "- Emit Company ONLY if it is the creator (CREATED_BY).\n",
    "- Emit EXACTLY ONE definition object for the primary entity.\n",
    "\n",
    "--------------------------------------------------\n",
    "WIKI-SPECIFIC: DEFINITION EXTRACTION (MANDATORY)\n",
    "--------------------------------------------------\n",
    "\n",
    "If and ONLY if the input source is a Wiki article, return ONE definition object.\n",
    "\n",
    "Definition MUST focus on UTILITY for career understanding.\n",
    "\n",
    "Schema:\n",
    "- definition: ONE sentence explaining what it is and why it is useful.\n",
    "- summary: 2 sentences explaining where and why it is used.\n",
    "- key_characteristics: 3â€“5 short, technical bullet points.\n",
    "- source: provenance string (e.g. \"wikipedia\").\n",
    "\n",
    "Do NOT include history, dates, or trivia.\n",
    "\n",
    "--------------------------------------------------\n",
    "ENTITY NORMALIZATION RULES\n",
    "--------------------------------------------------\n",
    "\n",
    "- Canonical names only (JS â†’ JavaScript).\n",
    "- Singular form only.\n",
    "- Title Case for entities.\n",
    "- Disambiguate using context.\n",
    "- Prefer CloudService over Platform for AWS, Azure, GCP.\n",
    "\n",
    "--------------------------------------------------\n",
    "OUTPUT FORMAT (STRICT JSON ONLY)\n",
    "--------------------------------------------------\n",
    "\n",
    "Return ONLY a valid JSON object. No markdown. No explanations.\n",
    "\n",
    "{\n",
    "  \"triplets\": [\n",
    "    {\n",
    "      \"subject\": \"string\",\n",
    "      \"subject_type\": \"string\",\n",
    "      \"relation\": \"string\",\n",
    "      \"object\": \"string\",\n",
    "      \"object_type\": \"string\"\n",
    "    }\n",
    "  ],\n",
    "  \"definitions\": [\n",
    "    {\n",
    "      \"name\": \"string\",\n",
    "      \"node_type\": \"string\",\n",
    "      \"definition\": \"string\",\n",
    "      \"summary\": \"string\",\n",
    "      \"key_characteristics\": [\"string\"],\n",
    "      \"source\": \"string\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztlnDHfS8Lhu"
   },
   "source": [
    "GROQ ANGLE(GPT-oss-120b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "ALf5eKGF8IJh",
    "outputId": "f25ca1ab-00a2-4ac1-deaf-730553009568"
   },
   "outputs": [],
   "source": [
    "\"\"\"import requests\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def main_gpt_oss():\n",
    "    print(\"ðŸ”§ Setting up OpenAI GPT-OSS-120B pipeline via Groq...\")\n",
    "\n",
    "    # 1. API Setup\n",
    "    # Use 'openai/gpt-oss-120b' for ultra-high reasoning\n",
    "    MODEL = \"openai/gpt-oss-120b\"\n",
    "    api_key = userdata.get('groq_key')\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # 2. Setup Generators\n",
    "    role_gen = DataIngestor.read_roles(Config.PATH_ROLES)\n",
    "    job_gen = DataIngestor.read_jobs_csv(Config.PATH_JOBS)\n",
    "    wiki_gen = DataIngestor.read_wiki(Config.PATH_WIKI)\n",
    "\n",
    "    # 3. Create Batcher\n",
    "    # REDUCED to 40k for 120B model due to strict TPM (Tokens Per Minute) limits\n",
    "    BATCH_SIZE = 40000\n",
    "    all_batches = batch_generator([role_gen, job_gen, wiki_gen], BATCH_SIZE)\n",
    "\n",
    "    print(f\"\\nðŸš€ Starting High-Reasoning Extraction (Resuming from Batch {Config.RESUME_FROM_BATCH})...\\n\")\n",
    "\n",
    "    batch_idx = 0\n",
    "    for batch_text in tqdm(all_batches, desc=\"120B Extraction\"):\n",
    "        batch_idx += 1\n",
    "\n",
    "        if batch_idx < Config.RESUME_FROM_BATCH:\n",
    "            if batch_idx % 10 == 0: print(f\"â© Skipping Batch {batch_idx}\")\n",
    "            continue\n",
    "\n",
    "        payload = {\n",
    "            \"model\": MODEL,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"EXTRACT KNOWLEDGE FROM THIS BATCH:\\n{batch_text}\"}\n",
    "            ],\n",
    "            \"response_format\": {\"type\": \"json_object\"},\n",
    "            \"temperature\": 0.1,\n",
    "            \"reasoning_effort\": \"high\" # Specific to GPT-OSS models for better logic\n",
    "        }\n",
    "\n",
    "        # --- RETRY LOGIC FOR THE 120B MODEL ---\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "                if response.status_code == 429:\n",
    "                    wait_time = 60 * (attempt + 1)\n",
    "                    print(f\"\\nâ³ Rate limit hit. Cooling down for {wait_time}s (Attempt {attempt+1}/{max_retries})...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Parse and Save\n",
    "                res_json = response.json()\n",
    "                raw_content = res_json['choices'][0]['message']['content']\n",
    "\n",
    "                if raw_content:\n",
    "                    json_data = json.loads(raw_content)\n",
    "\n",
    "                    with open(Config.LOCAL_OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n",
    "                        json.dump(json_data, f_out)\n",
    "                        f_out.write(\"\\n\")\n",
    "\n",
    "                    if os.path.exists(Config.DRIVE_BACKUP_DIR):\n",
    "                        shutil.copy2(Config.LOCAL_OUTPUT_FILE, Config.DRIVE_BACKUP_PATH)\n",
    "\n",
    "                    triplets = len(json_data.get('triplets', []))\n",
    "                    print(f\"âœ… Batch {batch_idx}: Saved {triplets} triplets (GPT-OSS 120B)\")\n",
    "\n",
    "                # SUCCESS: Wait for TPM to reset before next batch\n",
    "                # 80s is the 'Safe Zone' for the 120B model on free tiers\n",
    "                time.sleep(80)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ðŸ’€ Batch {batch_idx}: Error - {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(\"âŒ Max retries reached. Stopping to save progress.\")\n",
    "                    return\n",
    "                time.sleep(10)\n",
    "    print(f\"\\nðŸŽ‰ High-Reasoning session completed!\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGHujq2yg741"
   },
   "outputs": [],
   "source": [
    "class DataIngestor:\n",
    "    \"\"\"Reads raw files and yields formatted text strings.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def read_roles(path):\n",
    "        \"\"\"Reads the Detailed Job Descriptions JSON.\"\"\"\n",
    "        print(f\"Loading Roles from: {path}\")\n",
    "        try:\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                if not isinstance(data, list): data = [data]\n",
    "\n",
    "                for item in data:\n",
    "                    role_name = item.get(\"job_title\", item.get(\"role\", \"Unknown Role\"))\n",
    "                    yield (\n",
    "                        f\"SOURCE: ROLE_DEFINITION_JSON\\n\"\n",
    "                        f\"ROLE_NAME: {role_name}\\n\"\n",
    "                        f\"CONTENT: {json.dumps(item)}\\n\\n\"\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading roles: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def read_jobs_csv(path):\n",
    "        \"\"\"Reads the SerpAPI Job CSV.\"\"\"\n",
    "        print(f\"Loading Jobs from: {path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            cols = ['title', 'company_name', 'location', 'description']\n",
    "            for c in cols:\n",
    "                if c not in df.columns: df[c] = \"Unknown\"\n",
    "\n",
    "            for _, row in df[cols].iterrows():\n",
    "                # Jobs are already concise, keeping the 2500 char cap\n",
    "                desc = str(row['description'])[:5000]\n",
    "                text = (\n",
    "                    f\"SOURCE: JOB_DESCRIPTION_CSV\\n\"\n",
    "                    f\"Job Title: {row['title']}\\n\"\n",
    "                    f\"Company: {row['company_name']}\\n\"\n",
    "                    f\"Description: {desc}...\\n\\n\"\n",
    "                )\n",
    "                yield text\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading jobs: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def read_wiki(path):\n",
    "        \"\"\"Reads the Wiki JSONL file with a 1500 word cap per article.\"\"\"\n",
    "        print(f\"Loading Wiki from: {path}\")\n",
    "        try:\n",
    "            with open(path, 'r') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        item = json.loads(line)\n",
    "                        title = item.get('title', 'Unknown')\n",
    "                        content = item.get('text', '')\n",
    "\n",
    "                        # NEW: Word-based capping (Approx 6k-8k characters)\n",
    "                        # We split by space, take 1500 words, and join back.\n",
    "                        words = content.split()\n",
    "                        capped_content = \" \".join(words[:1500])\n",
    "\n",
    "                        yield (\n",
    "                            f\"SOURCE: WIKI_ARTICLE\\n\"\n",
    "                            f\"PRIMARY_ENTITY: {title}\\n\"\n",
    "                            f\"CONTENT: {capped_content}\\n\\n\"\n",
    "                        )\n",
    "                    except json.JSONDecodeError: continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading Wiki: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQcjGy7dMKcU"
   },
   "outputs": [],
   "source": [
    "def batch_generator(iterables, max_chars=53500):\n",
    "    \"\"\"\n",
    "    Combines generators into large batches (80k chars).\n",
    "    Ensures batches DO NOT mix SOURCE types.\n",
    "    \"\"\"\n",
    "    for source_gen in iterables:\n",
    "        current_batch = []\n",
    "        current_len = 0\n",
    "\n",
    "        for text_chunk in source_gen:\n",
    "            chunk_len = len(text_chunk)\n",
    "\n",
    "            # Safety for individual chunks larger than the total batch limit\n",
    "            if chunk_len > max_chars:\n",
    "                if current_batch:\n",
    "                    yield \"\".join(current_batch)\n",
    "                    current_batch = []\n",
    "                    current_len = 0\n",
    "                    yield text_chunk[:max_chars]\n",
    "                    continue\n",
    "\n",
    "            if current_len + chunk_len > max_chars and current_batch:\n",
    "                yield \"\".join(current_batch)\n",
    "                current_batch = []\n",
    "                current_len = 0\n",
    "\n",
    "            current_batch.append(text_chunk)\n",
    "            current_len += chunk_len\n",
    "\n",
    "        if current_batch:\n",
    "            yield \"\".join(current_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f-6Xw2WMKdN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ”§ Setting up pipeline...\")\n",
    "    client = genai.Client(api_key=Config.API_KEY)\n",
    "\n",
    "    # Setup Generators\n",
    "    role_gen = DataIngestor.read_roles(Config.PATH_ROLES)\n",
    "    job_gen = DataIngestor.read_jobs_csv(Config.PATH_JOBS)\n",
    "    wiki_gen = DataIngestor.read_wiki(Config.PATH_WIKI)\n",
    "\n",
    "    # Initialize Batcher with your 120k Config\n",
    "    all_batches = batch_generator([role_gen, job_gen, wiki_gen], Config.BATCH_SIZE_CHARS)\n",
    "\n",
    "    print(f\"\\nðŸš€ Extraction Pipeline (Resume: {Config.RESUME_FROM_BATCH} | Size: {Config.BATCH_SIZE_CHARS})...\\n\")\n",
    "\n",
    "    batch_idx = 0\n",
    "    for batch_text in tqdm(all_batches, desc=\"Batch Progress\"):\n",
    "        batch_idx += 1\n",
    "\n",
    "        if batch_idx < Config.RESUME_FROM_BATCH:\n",
    "            if batch_idx % 5 == 0: print(f\"â© Skipping Batch {batch_idx}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            full_content = f\"{SYSTEM_PROMPT}\\n\\n=== DATA TO EXTRACT ===\\n{batch_text}\"\n",
    "\n",
    "            response = client.models.generate_content(\n",
    "                model=Config.MODEL_NAME,\n",
    "                contents=full_content,\n",
    "                config=types.GenerateContentConfig(response_mime_type=\"application/json\")\n",
    "            )\n",
    "\n",
    "            if response.text:\n",
    "                try:\n",
    "                    json_data = json.loads(response.text)\n",
    "\n",
    "                    # 1. Atomic Local Save\n",
    "                    with open(Config.LOCAL_OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n",
    "                        json.dump(json_data, f_out)\n",
    "                        f_out.write(\"\\n\")\n",
    "\n",
    "                    # 2. Frequent Drive Sync (Every 2 batches to save overhead)\n",
    "                    if batch_idx % 2 == 0 and os.path.exists(Config.DRIVE_BACKUP_DIR):\n",
    "                        shutil.copy2(Config.LOCAL_OUTPUT_FILE, Config.DRIVE_BACKUP_PATH)\n",
    "\n",
    "                    triplet_count = len(json_data.get('triplets', []))\n",
    "                    print(f\"âœ… Batch {batch_idx}: {triplet_count} triplets saved.\")\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"âŒ Batch {batch_idx}: Invalid JSON response.\")\n",
    "\n",
    "            # FIXED: Fixed 15s sleep to respect 5 RPM and 250k TPM limits\n",
    "            time.sleep(20)\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) or \"RESOURCE_EXHAUSTED\" in str(e):\n",
    "                print(f\"\\nðŸ›‘ DAILY QUOTA (20 RPD) REACHED at Batch {batch_idx}. Resume tomorrow.\")\n",
    "                # Final emergency sync before stopping\n",
    "                shutil.copy2(Config.LOCAL_OUTPUT_FILE, Config.DRIVE_BACKUP_PATH)\n",
    "                break\n",
    "            else:\n",
    "                print(f\"ðŸ’€ Batch {batch_idx} Error: {str(e)}\")\n",
    "                time.sleep(30) # Extended cooldown on general errors\n",
    "\n",
    "    print(f\"\\nðŸŽ‰ Progress saved. Last processed: Batch {batch_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b4c3767971e74d1d8a7d8e5593550fc7",
      "85c05e71969545098cd8c62e2e957908",
      "a7b13e7120f64b0f9c9b0263326c55e2",
      "83f1d69af3bd4b8c9549d5797e1cf5b3",
      "78d02ab416ff4f08a2d9d7731970ad72",
      "d8dc0ea33800459ab3e337f1b0069f25",
      "8e47d8d120bd4581ba8551b119018897",
      "e3d2e41f56a94c7c8ba41f80b179a2a5",
      "cf8da96601344fe291ace94dc338135d",
      "9cf2ee5664f74cfea50b838c56ca5394",
      "ca3d9a322b0b42e484b4ac02661e51dc"
     ]
    },
    "id": "v3ZDNH8NMKeN",
    "outputId": "94f651ab-7271-4f3e-98a0-edd8a40d2d40"
   },
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUVeOfuFNAWC"
   },
   "source": [
    "PROCESSING TRIPLETS (VALIDATION LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEn_8SS5MKfI",
    "outputId": "e34a7f8c-e0e8-4d06-d563-f04a345f0d1b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "INPUT_FILE = \"knowledge_graph_extracted_DAY13.jsonl\"\n",
    "OUTPUT_FILE = \"knowledge_graph_clean_DAY13.jsonl\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. ONTOLOGY ENFORCEMENT (THE \"TRUTH\")\n",
    "# ==========================================\n",
    "\n",
    "# A. Type Overrides (Fixing LLM Misclassifications)\n",
    "TYPE_OVERRIDES = {\n",
    "    # Data Science / ML\n",
    "    \"Pandas\": \"Tool\", \"NumPy\": \"Tool\", \"Scikit-Learn\": \"Framework\",\n",
    "    \"Keras\": \"Framework\", \"PyTorch\": \"Framework\", \"TensorFlow\": \"Framework\",\n",
    "    \"OpenCV\": \"Tool\", \"NLTK\": \"Tool\", \"SpaCy\": \"Tool\",\n",
    "    \"Matplotlib\": \"Tool\", \"Seaborn\": \"Tool\", \"D3.js\": \"Tool\", \"MLflow\": \"Tool\",\n",
    "    # DevOps / Cloud\n",
    "    \"Docker\": \"Tool\", \"Kubernetes\": \"Tool\", \"Terraform\": \"Tool\",\n",
    "    \"Jenkins\": \"Tool\", \"Git\": \"Tool\", \"Ansible\": \"Tool\",\n",
    "    \"AWS\": \"CloudService\", \"Azure\": \"CloudService\", \"GCP\": \"CloudService\",\n",
    "    # Concepts\n",
    "    \"Statistics\": \"Concept\", \"Probability\": \"Concept\", \"Linear Algebra\": \"Concept\",\n",
    "    \"Calculus\": \"Concept\", \"Machine Learning\": \"Concept\", \"Deep Learning\": \"Concept\",\n",
    "    \"Data Science\": \"Concept\", \"Computer Vision\": \"Concept\", \"NLP\": \"Concept\",\n",
    "    \"CI/CD\": \"Concept\", \"DevOps\": \"Concept\", \"MLOps\": \"Concept\",\n",
    "    \"Agile\": \"Concept\", \"Scrum\": \"Concept\", \"REST\": \"Concept\",\n",
    "    \"Microservices\": \"Concept\", \"System Design\": \"Concept\", \"Algorithms\": \"Concept\",\n",
    "    \"Data Structures\": \"Concept\", \"Object-Oriented Programming\": \"Concept\"\n",
    "}\n",
    "\n",
    "# B. Canonical Name Mapping\n",
    "CANONICAL_NAMES = {\n",
    "    \"react.js\": \"React\", \"reactjs\": \"React\",\n",
    "    \"vue.js\": \"Vue\", \"vuejs\": \"Vue\", \"vue\": \"Vue\",\n",
    "    \"node.js\": \"Node.js\", \"nodejs\": \"Node.js\", \"node\": \"Node.js\",\n",
    "    \"express.js\": \"Express\", \"expressjs\": \"Express\",\n",
    "    \"next.js\": \"Next.js\", \"nextjs\": \"Next.js\",\n",
    "    \"amazon web services\": \"AWS\", \"aws\": \"AWS\",\n",
    "    \"google cloud platform\": \"GCP\", \"google cloud\": \"GCP\", \"gcp\": \"GCP\",\n",
    "    \"microsoft azure\": \"Azure\", \"azure\": \"Azure\",\n",
    "    \"postgresql\": \"PostgreSQL\", \"postgres\": \"PostgreSQL\",\n",
    "    \"mongodb\": \"MongoDB\", \"mongo\": \"MongoDB\",\n",
    "    \"mssql\": \"SQL Server\", \"sql server\": \"SQL Server\",\n",
    "    \"tensorflow\": \"TensorFlow\", \"tf\": \"TensorFlow\",\n",
    "    \"pytorch\": \"PyTorch\",\n",
    "    \"scikit-learn\": \"Scikit-Learn\", \"sklearn\": \"Scikit-Learn\",\n",
    "    \"mlflow\": \"MLflow\",\n",
    "    \"golang\": \"Go\", \"go lang\": \"Go\",\n",
    "    \"c#\": \"C#\", \"c sharp\": \"C#\", \"c++\": \"C++\",\n",
    "    \"dotnet\": \".NET\", \".net\": \".NET\", \".net core\": \".NET Core\"\n",
    "}\n",
    "\n",
    "# C. Patterns to Strip\n",
    "BAD_PATTERNS = [\n",
    "    r\"\\bability to\\b\", r\"\\bexperience in\\b\", r\"\\bknowledge of\\b\",\n",
    "    r\"\\bresponsible for\\b\", r\"\\bworking with\\b\", r\"\\bunderstanding of\\b\",\n",
    "    r\"\\bfamiliarity with\\b\", r\"\\bproven track record\\b\", r\"\\bproficient in\\b\"\n",
    "]\n",
    "\n",
    "ADJECTIVES = [\n",
    "    r\"^Strong\\s+\", r\"^Advanced\\s+\", r\"^Basic\\s+\", r\"^Senior\\s+\", r\"^Junior\\s+\",\n",
    "    r\"^Hands-on\\s+\", r\"^Expertise in\\s+\", r\"\\s+Skills$\", r\"\\s+Development$\"\n",
    "]\n",
    "\n",
    "# D. Injected Logic\n",
    "INJECTED_EDGES = {\n",
    "    \"PyTorch\": [(\"IMPLEMENTS_CONCEPT\", \"Deep Learning\", \"Concept\")],\n",
    "    \"TensorFlow\": [(\"IMPLEMENTS_CONCEPT\", \"Deep Learning\", \"Concept\")],\n",
    "    \"Scikit-Learn\": [(\"IMPLEMENTS_CONCEPT\", \"Machine Learning\", \"Concept\")],\n",
    "    \"React\": [(\"IMPLEMENTS_CONCEPT\", \"Frontend Development\", \"Concept\"), (\"IMPLEMENTS_CONCEPT\", \"Component-Based Architecture\", \"Concept\")],\n",
    "    \"Docker\": [(\"IMPLEMENTS_CONCEPT\", \"Containerization\", \"Concept\")],\n",
    "    \"Kubernetes\": [(\"IMPLEMENTS_CONCEPT\", \"Orchestration\", \"Concept\")],\n",
    "    \"PostgreSQL\": [(\"IMPLEMENTS_CONCEPT\", \"Relational Database\", \"Concept\")],\n",
    "    \"MongoDB\": [(\"IMPLEMENTS_CONCEPT\", \"NoSQL\", \"Concept\")],\n",
    "    \"REST\": [(\"IS_SUBTOPIC_OF\", \"Architectural Style\", \"Concept\")],\n",
    "    \"CI/CD\": [(\"IS_SUBTOPIC_OF\", \"DevOps Practice\", \"Concept\")]\n",
    "}\n",
    "\n",
    "ALLOWED_PATTERNS = {\n",
    "    (\"JobRole\", \"REQUIRES_SKILL\", \"Skill\"),\n",
    "    (\"JobRole\", \"REQUIRES_KNOWLEDGE\", \"Concept\"),\n",
    "    (\"JobRole\", \"REQUIRES_CONCEPT\", \"Concept\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"Tool\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"Framework\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"Database\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"Platform\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"CloudService\"),\n",
    "    (\"JobRole\", \"REQUIRES_LANGUAGE\", \"ProgrammingLanguage\"),\n",
    "    (\"Tool\", \"IMPLEMENTS_CONCEPT\", \"Concept\"),\n",
    "    (\"Framework\", \"IMPLEMENTS_CONCEPT\", \"Concept\"),\n",
    "    (\"ProgrammingLanguage\", \"IMPLEMENTS_CONCEPT\", \"Concept\"),\n",
    "    (\"Tool\", \"USES_LANGUAGE\", \"ProgrammingLanguage\"),\n",
    "    (\"Framework\", \"USES_LANGUAGE\", \"ProgrammingLanguage\"),\n",
    "    (\"Concept\", \"IS_SUBTOPIC_OF\", \"Concept\")\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "# 3. CLEANING LOGIC\n",
    "# ==========================================\n",
    "def clean_name(name):\n",
    "    if not name: return None\n",
    "    name = name.strip()\n",
    "\n",
    "    # 1. Strip Adjectives\n",
    "    for pattern in ADJECTIVES:\n",
    "        name = re.sub(pattern, \"\", name, flags=re.IGNORECASE)\n",
    "\n",
    "    # 2. Check for Vague Phrases\n",
    "    for pat in BAD_PATTERNS:\n",
    "        if re.search(pat, name.lower()):\n",
    "            return None\n",
    "\n",
    "    # 3. Canonicalize\n",
    "    lower_name = name.lower()\n",
    "    if lower_name in CANONICAL_NAMES:\n",
    "        return CANONICAL_NAMES[lower_name]\n",
    "\n",
    "    # 4. Default Title Case\n",
    "    if lower_name in [\"sql\", \"api\", \"etl\", \"elt\", \"bi\"]:\n",
    "        return name.upper()\n",
    "\n",
    "    return name.title()\n",
    "\n",
    "def validate_triplet(t):\n",
    "    # 1. Clean Names (Defensive check for missing keys)\n",
    "    s_raw = t.get('subject')\n",
    "    o_raw = t.get('object')\n",
    "\n",
    "    if not s_raw or not o_raw: return None\n",
    "\n",
    "    s_clean = clean_name(s_raw)\n",
    "    o_clean = clean_name(o_raw)\n",
    "\n",
    "    if not s_clean or not o_clean: return None\n",
    "\n",
    "    # 2. Apply Type Overrides\n",
    "    # FIX: Use .get() with a default value to prevent KeyError\n",
    "    s_type_raw = t.get('subject_type', 'Concept')\n",
    "    o_type_raw = t.get('object_type', 'Concept')\n",
    "\n",
    "    s_type = TYPE_OVERRIDES.get(s_clean, s_type_raw)\n",
    "    o_type = TYPE_OVERRIDES.get(o_clean, o_type_raw)\n",
    "\n",
    "    # 3. Subject-Side Coercion\n",
    "    if s_clean in TYPE_OVERRIDES and TYPE_OVERRIDES[s_clean] == \"Concept\":\n",
    "        s_type = \"Concept\"\n",
    "\n",
    "    # 4. Strict Logic: Concepts cannot be Skills\n",
    "    relation = t.get('relation', 'RELATED_TO') # Default relation if missing\n",
    "    if o_clean in TYPE_OVERRIDES and TYPE_OVERRIDES[o_clean] == \"Concept\":\n",
    "        if relation == \"REQUIRES_SKILL\":\n",
    "            relation = \"REQUIRES_CONCEPT\"\n",
    "\n",
    "    return {\n",
    "        \"subject\": s_clean, \"subject_type\": s_type,\n",
    "        \"relation\": relation,\n",
    "        \"object\": o_clean, \"object_type\": o_type\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN PROCESSING\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"ðŸ§¹ Starting V3 Canonical Validation (Defensive Mode)...\")\n",
    "\n",
    "    seen_hashes = set()\n",
    "    valid_count = 0\n",
    "\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r') as f_in, open(OUTPUT_FILE, 'w') as f_out:\n",
    "\n",
    "            for line in f_in:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    triplets = data.get(\"triplets\", [])\n",
    "\n",
    "                    if not isinstance(triplets, list): continue # Guard against malformed JSON\n",
    "\n",
    "                    for raw in triplets:\n",
    "                        clean_t = validate_triplet(raw)\n",
    "                        if not clean_t: continue\n",
    "\n",
    "                        # --- WRITE ORIGINAL ---\n",
    "                        h = f\"{clean_t['subject']}|{clean_t['relation']}|{clean_t['object']}\"\n",
    "                        if h not in seen_hashes:\n",
    "                            f_out.write(json.dumps(clean_t) + \"\\n\")\n",
    "                            seen_hashes.add(h)\n",
    "                            valid_count += 1\n",
    "\n",
    "                        # --- HYDRATION (INJECT MISSING EDGES) ---\n",
    "                        if clean_t['subject'] in INJECTED_EDGES:\n",
    "                            for rel, target, target_type in INJECTED_EDGES[clean_t['subject']]:\n",
    "                                if (clean_t['subject_type'], rel, target_type) not in ALLOWED_PATTERNS:\n",
    "                                    if rel not in [\"IMPLEMENTS_CONCEPT\", \"IS_SUBTOPIC_OF\"]:\n",
    "                                        continue\n",
    "\n",
    "                                new_t = {\n",
    "                                    \"subject\": clean_t['subject'], \"subject_type\": clean_t['subject_type'],\n",
    "                                    \"relation\": rel,\n",
    "                                    \"object\": target, \"object_type\": target_type\n",
    "                                }\n",
    "                                h_new = f\"{new_t['subject']}|{new_t['relation']}|{new_t['object']}\"\n",
    "                                if h_new not in seen_hashes:\n",
    "                                    f_out.write(json.dumps(new_t) + \"\\n\")\n",
    "                                    seen_hashes.add(h_new)\n",
    "                                    valid_count += 1\n",
    "\n",
    "                        if clean_t['object'] in INJECTED_EDGES:\n",
    "                            for rel, target, target_type in INJECTED_EDGES[clean_t['object']]:\n",
    "                                new_t = {\n",
    "                                    \"subject\": clean_t['object'], \"subject_type\": clean_t['object_type'],\n",
    "                                    \"relation\": rel,\n",
    "                                    \"object\": target, \"object_type\": target_type\n",
    "                                }\n",
    "                                h_new = f\"{new_t['subject']}|{new_t['relation']}|{new_t['object']}\"\n",
    "                                if h_new not in seen_hashes:\n",
    "                                    f_out.write(json.dumps(new_t) + \"\\n\")\n",
    "                                    seen_hashes.add(h_new)\n",
    "                                    valid_count += 1\n",
    "\n",
    "                except json.JSONDecodeError: pass\n",
    "\n",
    "        print(f\"âœ… Success! Written {valid_count} canonicalized triplets to {OUTPUT_FILE}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Input file '{INPUT_FILE}' not found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwpH6tk4Yq24"
   },
   "source": [
    "GENERATING CYPHERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huW2SqFnMKgE",
    "outputId": "ec1992b8-01ea-4f1b-e4f8-c681a0d1106f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "INPUT_FILE = \"knowledge_graph_clean_DAY13.jsonl\"\n",
    "OUTPUT_FILE = \"ingest_graph_DAY13.cypher\"\n",
    "BATCH_SIZE = 1000  # Safe batch size for AuraDB\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER: CHUNKING\n",
    "# ==========================================\n",
    "def chunker(seq, size):\n",
    "    \"\"\"Yields chunks of a specific size from a list.\"\"\"\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield seq[i:i + size]\n",
    "\n",
    "# ==========================================\n",
    "# 3. GENERATORS\n",
    "# ==========================================\n",
    "def generate_constraints(types):\n",
    "    \"\"\"Generates unique constraints for every node type found.\"\"\"\n",
    "    return [\n",
    "        f\"CREATE CONSTRAINT {t.lower()}_uniq IF NOT EXISTS FOR (n:{t}) REQUIRE n.name IS UNIQUE;\"\n",
    "        for t in sorted(list(types))\n",
    "    ]\n",
    "\n",
    "def generate_node_cypher(triplets):\n",
    "    \"\"\"Generates batched UNWIND statements for Node creation.\"\"\"\n",
    "    nodes = {}\n",
    "\n",
    "    # 1. Aggregate unique names per type\n",
    "    for t in triplets:\n",
    "        for side in ['subject', 'object']:\n",
    "            typ = t[f'{side}_type']\n",
    "            name = t[f'{side}']\n",
    "            if typ not in nodes: nodes[typ] = set()\n",
    "            nodes[typ].add(name)\n",
    "\n",
    "    statements = []\n",
    "    statements.append(\"// --- 2. NODE CREATION ---\")\n",
    "\n",
    "    # 2. Generate Batched Queries\n",
    "    for n_type, names_set in nodes.items():\n",
    "        all_names = list(names_set)\n",
    "\n",
    "        for batch in chunker(all_names, BATCH_SIZE):\n",
    "            json_batch = json.dumps(batch)  # Handles escaping safely\n",
    "\n",
    "            query = (\n",
    "                f\"// Batch: Create {n_type} nodes\\n\"\n",
    "                f\"UNWIND {json_batch} AS name \"\n",
    "                f\"MERGE (n:{n_type} {{name: name}});\"\n",
    "            )\n",
    "            statements.append(query)\n",
    "\n",
    "    return statements\n",
    "\n",
    "def generate_rel_cypher(triplets):\n",
    "    \"\"\"Generates batched UNWIND statements for Relationship creation.\"\"\"\n",
    "    rels = {}\n",
    "\n",
    "    # 1. Aggregate relations by signature\n",
    "    for t in triplets:\n",
    "        key = (t['subject_type'], t['relation'], t['object_type'])\n",
    "        if key not in rels: rels[key] = []\n",
    "\n",
    "        rels[key].append({\"s\": t['subject'], \"o\": t['object']})\n",
    "\n",
    "    statements = []\n",
    "    statements.append(\"// --- 3. RELATIONSHIP CREATION ---\")\n",
    "\n",
    "    # 2. Generate Batched Queries\n",
    "    for (st, rel, ot), pairs in rels.items():\n",
    "        for batch in chunker(pairs, BATCH_SIZE):\n",
    "            json_batch = json.dumps(batch)\n",
    "\n",
    "            query = (\n",
    "                f\"// Batch: {st} -> {rel} -> {ot}\\n\"\n",
    "                f\"UNWIND {json_batch} AS row \"\n",
    "                f\"MATCH (s:{st} {{name: row.s}}) \"\n",
    "                f\"MATCH (o:{ot} {{name: row.o}}) \"\n",
    "                f\"MERGE (s)-[:{rel}]->(o);\"\n",
    "            )\n",
    "            statements.append(query)\n",
    "\n",
    "    return statements\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"âš™ï¸ Generating Optimized Cypher Script...\")\n",
    "    triplets = []\n",
    "\n",
    "    try:\n",
    "        # Load JSONL\n",
    "        with open(INPUT_FILE, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        triplets.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "        print(f\"ðŸ“Š Loaded {len(triplets)} triplets.\")\n",
    "\n",
    "        # 1. Generate Components\n",
    "        all_types = set()\n",
    "        for t in triplets:\n",
    "            all_types.add(t['subject_type'])\n",
    "            all_types.add(t['object_type'])\n",
    "\n",
    "        constraints = generate_constraints(all_types)\n",
    "        node_queries = generate_node_cypher(triplets)\n",
    "        rel_queries = generate_rel_cypher(triplets)\n",
    "\n",
    "        # 2. Write to File with Instructions\n",
    "        with open(OUTPUT_FILE, 'w') as f:\n",
    "            # Header Instructions\n",
    "            f.write(\"// ======================================================\\n\")\n",
    "            f.write(\"// INSTRUCTIONS FOR NEO4J BROWSER:\\n\")\n",
    "            f.write(\"// 1. Ensure 'Connect result nodes' is unchecked in settings (optional, for speed).\\n\")\n",
    "            f.write(\"// 2. Ensure 'Enable multi-statement query editor' is CHECKED.\\n\")\n",
    "            f.write(\"// 3. Copy-paste this entire file and run it.\\n\")\n",
    "            f.write(\"// ======================================================\\n\\n\")\n",
    "\n",
    "            f.write(\"// --- 1. CONSTRAINTS ---\\n\")\n",
    "            f.write(\"\\n\".join(constraints) + \"\\n\\n\")\n",
    "\n",
    "            # Double newline (\\n\\n) ensures clear separation for the Browser parser\n",
    "            f.write(\"\\n\\n\".join(node_queries) + \"\\n\\n\")\n",
    "            f.write(\"\\n\\n\".join(rel_queries) + \"\\n\")\n",
    "\n",
    "        print(f\"âœ… Success! Script saved to: {OUTPUT_FILE}\")\n",
    "        print(f\"   - Node Batches: {len(node_queries)}\")\n",
    "        print(f\"   - Relationship Batches: {len(rel_queries)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Error: Input file not found. Run validation script first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPbq0SMscnV5"
   },
   "source": [
    "PUSHING CYPHERS TO GRAPH DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9AV5CdyiFgG",
    "outputId": "133e9a0f-31f5-4960-ce82-9b19e8ab2dd6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "SOURCE_DIR = \"/content/drive/MyDrive/rr_mlops\"  # Where your .jsonl files are\n",
    "OUTPUT_FILE = os.path.join(SOURCE_DIR, \"ingest_fixed.cypher\")\n",
    "BATCH_SIZE = 1000  # Smaller batch size = Safer ingestion\n",
    "\n",
    "def generate_clean_cypher():\n",
    "    print(f\"ðŸ§¹ Scanning {SOURCE_DIR} for JSONL files...\")\n",
    "    files = glob.glob(os.path.join(SOURCE_DIR, \"*.jsonl\"))\n",
    "\n",
    "    if not files:\n",
    "        print(\"âŒ No .jsonl files found! Cannot regenerate.\")\n",
    "        return\n",
    "\n",
    "    # 1. Collect Valid Triplets\n",
    "    # We use a set to deduplicate inherently\n",
    "    unique_triplets = set()\n",
    "\n",
    "    for f_path in files:\n",
    "        print(f\"   ðŸ“– Reading {os.path.basename(f_path)}...\")\n",
    "        with open(f_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    # Extract triplets if valid\n",
    "                    if \"triplets\" in data:\n",
    "                        for t in data[\"triplets\"]:\n",
    "                            # Create a tuple signature to deduplicate: (sub, sub_type, rel, obj, obj_type)\n",
    "                            sig = (\n",
    "                                t.get(\"subject\"),\n",
    "                                t.get(\"subject_type\"),\n",
    "                                t.get(\"relation\"),\n",
    "                                t.get(\"object\"),\n",
    "                                t.get(\"object_type\")\n",
    "                            )\n",
    "                            if all(sig): # Ensure no None values\n",
    "                                unique_triplets.add(sig)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    print(f\"âœ… Loaded {len(unique_triplets)} unique triplets.\")\n",
    "\n",
    "    # 2. Structure Data for Batching\n",
    "    # We separate by Relation Type to optimize Neo4j locking\n",
    "    # Structure: { \"REQUIRES_SKILL\": [ {s:..., o:...}, ... ], ... }\n",
    "    batched_rels = {}\n",
    "    nodes = set() # (name, type)\n",
    "\n",
    "    for sub, sub_type, rel, obj, obj_type in unique_triplets:\n",
    "        # Collect Nodes\n",
    "        nodes.add((sub, sub_type))\n",
    "        nodes.add((obj, obj_type))\n",
    "\n",
    "        # Collect Edge Data\n",
    "        if rel not in batched_rels:\n",
    "            batched_rels[rel] = []\n",
    "\n",
    "        # We store just the names and types needed for the match\n",
    "        batched_rels[rel].append({\n",
    "            \"s\": sub, \"st\": sub_type,\n",
    "            \"o\": obj, \"ot\": obj_type\n",
    "        })\n",
    "\n",
    "    # 3. Write the Clean Cypher Script\n",
    "    print(f\"âœï¸ Writing to {OUTPUT_FILE}...\")\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"// AUTO-GENERATED VALIDATED CYPHER\\n\\n\")\n",
    "\n",
    "        # --- PART A: NODES (Batched) ---\n",
    "        # Group nodes by label for efficient UNWIND\n",
    "        nodes_by_label = {}\n",
    "        for name, label in nodes:\n",
    "            if label not in nodes_by_label: nodes_by_label[label] = []\n",
    "            nodes_by_label[label].append(name)\n",
    "\n",
    "        for label, names in nodes_by_label.items():\n",
    "            # Chunk into batches\n",
    "            for i in range(0, len(names), BATCH_SIZE):\n",
    "                batch = names[i:i+BATCH_SIZE]\n",
    "                # We use json.dumps to ensure quotes/escaping are perfect\n",
    "                json_batch = json.dumps(batch)\n",
    "\n",
    "                query = (\n",
    "                    f\"// Create {label} Nodes (Batch {i})\\n\"\n",
    "                    f\"UNWIND {json_batch} AS name\\n\"\n",
    "                    f\"MERGE (n:`{label}` {{name: name}});\\n\\n\"\n",
    "                )\n",
    "                f.write(query)\n",
    "\n",
    "        # --- PART B: RELATIONSHIPS (Batched) ---\n",
    "        for rel_type, edges in batched_rels.items():\n",
    "            for i in range(0, len(edges), BATCH_SIZE):\n",
    "                batch = edges[i:i+BATCH_SIZE]\n",
    "                json_batch = json.dumps(batch)\n",
    "\n",
    "                # Dynamic Cypher generation based on the first item's types\n",
    "                # (Assuming uniform types per relation batch for simplicity,\n",
    "                # but we match on s/st to be safe)\n",
    "\n",
    "                query = (\n",
    "                    f\"// Create {rel_type} Edges (Batch {i})\\n\"\n",
    "                    f\"UNWIND {json_batch} AS row\\n\"\n",
    "                    f\"MATCH (s {{name: row.s}}) WHERE labels(s)[0] = row.st\\n\"\n",
    "                    f\"MATCH (o {{name: row.o}}) WHERE labels(o)[0] = row.ot\\n\"\n",
    "                    f\"MERGE (s)-[:`{rel_type}`]->(o);\\n\\n\"\n",
    "                )\n",
    "                f.write(query)\n",
    "\n",
    "    print(\"ðŸŽ‰ Success! New script is ready for ingestion.\")\n",
    "\n",
    "# Run it\n",
    "generate_clean_cypher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Hn-c2geMKhB",
    "outputId": "5b8a555a-d94e-4516-83ab-ec41643a9118"
   },
   "outputs": [],
   "source": [
    "!pip install neo4j~=5.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7oMefuMcrax"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "NEO4J_PASSWORD= userdata.get('neo4j_rr_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7E19RNQega0x",
    "outputId": "1b6d9237-3a5a-4e1b-a0c2-c31acd9b6665"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from neo4j import GraphDatabase, exceptions\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    SOURCE_DIR = \"/content/drive/MyDrive/rr_mlops\"\n",
    "\n",
    "    # DB Credentials\n",
    "    NEO4J_URI = \"neo4j+s://0c60999b.databases.neo4j.io\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD= NEO4J_PASSWORD\n",
    "\n",
    "    BATCH_SIZE = 1000\n",
    "    MAX_RETRIES = 3\n",
    "\n",
    "    # All node types that need unique constraints\n",
    "    ONTOLOGY_LABELS = [\n",
    "        \"JobPosting\", \"JobRole\", \"Skill\", \"Concept\",\n",
    "        \"ProgrammingLanguage\", \"Framework\", \"Tool\",\n",
    "        \"Platform\", \"CloudService\", \"Database\",\n",
    "        \"Company\", \"Location\"\n",
    "    ]\n",
    "\n",
    "# ==========================================\n",
    "# 2. ROBUST UTILS\n",
    "# ==========================================\n",
    "def run_with_retry(session, query, parameters, retries=Config.MAX_RETRIES):\n",
    "    \"\"\"Executes a query with transient error handling.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            result = session.run(query, parameters)\n",
    "            return result.consume()\n",
    "        except (exceptions.ServiceUnavailable, exceptions.TransientError) as e:\n",
    "            if attempt < retries - 1:\n",
    "                sleep_time = 2 * (attempt + 1)\n",
    "                print(f\"      âš ï¸ Transient Error (Attempt {attempt+1}/{retries}). Retrying in {sleep_time}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# ==========================================\n",
    "# 3. SCHEMA SAFETY (Constraints)\n",
    "# ==========================================\n",
    "def ensure_constraints(driver):\n",
    "    print(\"\\nðŸ›¡ï¸ Verifying Constraints (Speed & Integrity)...\")\n",
    "    with driver.session() as session:\n",
    "        for label in Config.ONTOLOGY_LABELS:\n",
    "            # We use the idempotent syntax (IF NOT EXISTS)\n",
    "            query = f\"CREATE CONSTRAINT unique_{label.lower()} IF NOT EXISTS FOR (n:`{label}`) REQUIRE n.name IS UNIQUE\"\n",
    "            try:\n",
    "                session.run(query)\n",
    "                print(f\"   âœ… Constraint active: :{label}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Failed to set constraint for :{label} -> {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. DATA PREPARATION (Signature Grouping)\n",
    "# ==========================================\n",
    "def load_and_group_data(source_dir):\n",
    "    print(f\"\\nðŸ“– Scanning {source_dir} for cleaned data...\")\n",
    "    # SORTED for determinism\n",
    "    files = sorted(glob.glob(os.path.join(source_dir, \"*.jsonl\")))\n",
    "\n",
    "    # 1. Group Nodes by Label: { \"JobRole\": {\"DevOps\", \"SRE\"}, ... }\n",
    "    nodes_by_label = {}\n",
    "\n",
    "    # 2. Group Edges by Signature: { (\"JobRole\", \"REQUIRES_SKILL\", \"Skill\"): [{\"s\": \"DevOps\", \"o\": \"Docker\"}, ...], ... }\n",
    "    edges_by_signature = {}\n",
    "\n",
    "    triplet_count = 0\n",
    "\n",
    "    for f_path in files:\n",
    "        # Skip raw extraction logs if you have clean/final files\n",
    "        if \"failed\" in f_path: continue\n",
    "\n",
    "        with open(f_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    if \"triplets\" in data:\n",
    "                        for t in data[\"triplets\"]:\n",
    "                            s, st = t.get(\"subject\"), t.get(\"subject_type\")\n",
    "                            r = t.get(\"relation\")\n",
    "                            o, ot = t.get(\"object\"), t.get(\"object_type\")\n",
    "\n",
    "                            if all([s, st, r, o, ot]):\n",
    "                                # Add to Nodes\n",
    "                                nodes_by_label.setdefault(st, set()).add(s)\n",
    "                                nodes_by_label.setdefault(ot, set()).add(o)\n",
    "\n",
    "                                # Add to Edges (Signature Based)\n",
    "                                sig = (st, r, ot)\n",
    "                                edges_by_signature.setdefault(sig, []).append({\"s\": s, \"o\": o})\n",
    "                                triplet_count += 1\n",
    "                except: continue\n",
    "\n",
    "    print(f\"âœ… Parsed {triplet_count} triplets.\")\n",
    "    print(f\"   - {len(nodes_by_label)} Node Labels found\")\n",
    "    print(f\"   - {len(edges_by_signature)} Unique Relationship Patterns found\")\n",
    "\n",
    "    return nodes_by_label, edges_by_signature\n",
    "\n",
    "# ==========================================\n",
    "# 5. INGESTION ENGINE\n",
    "# ==========================================\n",
    "def ingest_data(driver, nodes_map, edges_map):\n",
    "    with driver.session() as session:\n",
    "\n",
    "        # --- PHASE 1: NODES (The Foundation) ---\n",
    "        print(\"\\nðŸ—ï¸ Phase 1: Ingesting Nodes...\")\n",
    "        for label, names in nodes_map.items():\n",
    "            name_list = list(names)\n",
    "            total = len(name_list)\n",
    "            print(f\"   ðŸ”¹ :{label} ({total} items)\")\n",
    "\n",
    "            for i in range(0, total, Config.BATCH_SIZE):\n",
    "                batch = name_list[i:i+Config.BATCH_SIZE]\n",
    "\n",
    "                # QUERY: Hardcoded Label -> Fast & Safe\n",
    "                query = f\"UNWIND $batch AS name MERGE (n:`{label}` {{name: name}})\"\n",
    "\n",
    "                try:\n",
    "                    run_with_retry(session, query, {\"batch\": batch})\n",
    "                except Exception as e:\n",
    "                    print(f\"      âŒ Failed batch {i}: {e}\")\n",
    "\n",
    "        # --- PHASE 2: RELATIONSHIPS (The Context) ---\n",
    "        print(\"\\nðŸ”— Phase 2: Ingesting Relationships...\")\n",
    "        for (s_label, rel_type, o_label), edge_list in edges_map.items():\n",
    "            total = len(edge_list)\n",
    "            print(f\"   ðŸ”¸ ({s_label}) -[:{rel_type}]-> ({o_label}) : {total} items\")\n",
    "\n",
    "            for i in range(0, total, Config.BATCH_SIZE):\n",
    "                batch = edge_list[i:i+Config.BATCH_SIZE]\n",
    "\n",
    "                # QUERY: Dynamic Construction, Static Execution\n",
    "                # This fixes the \"labels(s)[0]\" bug by knowing the label in advance\n",
    "                query = (\n",
    "                    f\"UNWIND $batch AS row \"\n",
    "                    f\"MATCH (s:`{s_label}` {{name: row.s}}) \"\n",
    "                    f\"MATCH (o:`{o_label}` {{name: row.o}}) \"\n",
    "                    f\"MERGE (s)-[:`{rel_type}`]->(o)\"\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    run_with_retry(session, query, {\"batch\": batch})\n",
    "                except Exception as e:\n",
    "                    print(f\"      âŒ Failed batch {i}: {e}\")\n",
    "\n",
    "    print(\"\\nðŸŽ‰ Ingestion Pipeline Complete!\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = GraphDatabase.driver(\n",
    "            Config.NEO4J_URI,\n",
    "            auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD)\n",
    "        )\n",
    "        driver.verify_connectivity()\n",
    "\n",
    "        # 1. Safety\n",
    "        ensure_constraints(driver)\n",
    "\n",
    "        # 2. Preparation\n",
    "        nodes, edges = load_and_group_data(Config.SOURCE_DIR)\n",
    "\n",
    "        # 3. Execution\n",
    "        ingest_data(driver, nodes, edges)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ’€ Critical Error: {e}\")\n",
    "    finally:\n",
    "        if driver: driver.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVSMVjpr1eOt"
   },
   "source": [
    "\n",
    "ðŸ›¡ï¸ Verifying Constraints (Speed & Integrity)...\n",
    "   âœ… Constraint active: :JobPosting\n",
    "   âœ… Constraint active: :JobRole\n",
    "   âœ… Constraint active: :Skill\n",
    "   âœ… Constraint active: :Concept\n",
    "   âœ… Constraint active: :ProgrammingLanguage\n",
    "   âœ… Constraint active: :Framework\n",
    "   âœ… Constraint active: :Tool\n",
    "   âœ… Constraint active: :Platform\n",
    "   âœ… Constraint active: :CloudService\n",
    "   âœ… Constraint active: :Database\n",
    "   âœ… Constraint active: :Company\n",
    "   âœ… Constraint active: :Location\n",
    "\n",
    "ðŸ“– Scanning /content/drive/MyDrive/rr_mlops for cleaned data...\n",
    "âœ… Parsed 35015 triplets.\n",
    "   - 13 Node Labels found\n",
    "   - 140 Unique Relationship Patterns found\n",
    "\n",
    "ðŸ—ï¸ Phase 1: Ingesting Nodes...\n",
    "   ðŸ”¹ :JobRole (463 items)\n",
    "   ðŸ”¹ :ProgrammingLanguage (239 items)\n",
    "   ðŸ”¹ :Framework (578 items)\n",
    "   ðŸ”¹ :Tool (1628 items)\n",
    "   ðŸ”¹ :Database (147 items)\n",
    "   ðŸ”¹ :CloudService (293 items)\n",
    "   ðŸ”¹ :Skill (4361 items)\n",
    "   ðŸ”¹ :Concept (5219 items)\n",
    "   ðŸ”¹ :Platform (329 items)\n",
    "   ðŸ”¹ :Company (668 items)\n",
    "   ðŸ”¹ :null (1 items)\n",
    "   ðŸ”¹ :Location (164 items)\n",
    "   ðŸ”¹ :JobPosting (585 items)\n",
    "\n",
    "ðŸ”— Phase 2: Ingesting Relationships...\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_LANGUAGE]-> (ProgrammingLanguage) : 1309 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_TOOL]-> (Framework) : 921 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_TOOL]-> (Tool) : 2192 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_TOOL]-> (Database) : 524 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_TOOL]-> (CloudService) : 677 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_SKILL]-> (Skill) : 7280 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_CONCEPT]-> (Concept) : 8733 items\n",
    "   ðŸ”¸ (JobRole) -[:RELATED_ROLE]-> (JobRole) : 347 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 1143 items\n",
    "   ðŸ”¸ (Framework) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 1573 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_FRAMEWORK]-> (Framework) : 321 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_DATABASE]-> (Database) : 152 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_PLATFORM]-> (Platform) : 129 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_CLOUDSERVICE]-> (CloudService) : 106 items\n",
    "   ðŸ”¸ (Framework) -[:USES_LANGUAGE]-> (ProgrammingLanguage) : 605 items\n",
    "   ðŸ”¸ (Tool) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 2127 items\n",
    "   ðŸ”¸ (Tool) -[:USES_LANGUAGE]-> (ProgrammingLanguage) : 220 items\n",
    "   ðŸ”¸ (Database) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 368 items\n",
    "   ðŸ”¸ (CloudService) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 388 items\n",
    "   ðŸ”¸ (Platform) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 400 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_CLOUD_SERVICE]-> (CloudService) : 104 items\n",
    "   ðŸ”¸ (Tool) -[:IS_SIMILAR_TO]-> (Tool) : 196 items\n",
    "   ðŸ”¸ (Database) -[:IS_SIMILAR_TO]-> (Database) : 54 items\n",
    "   ðŸ”¸ (CloudService) -[:IS_SIMILAR_TO]-> (CloudService) : 22 items\n",
    "   ðŸ”¸ (Framework) -[:CREATED_BY]-> (Company) : 154 items\n",
    "   ðŸ”¸ (Framework) -[:IS_SIMILAR_TO]-> (Framework) : 219 items\n",
    "   ðŸ”¸ (Tool) -[:CREATED_BY]-> (Company) : 316 items\n",
    "   ðŸ”¸ (Platform) -[:CREATED_BY]-> (Company) : 57 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:CREATED_BY]-> (Company) : 65 items\n",
    "   ðŸ”¸ (CloudService) -[:CREATED_BY]-> (Company) : 56 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_TOOL]-> (Platform) : 239 items\n",
    "   ðŸ”¸ (Database) -[:CREATED_BY]-> (Company) : 26 items\n",
    "   ðŸ”¸ (Skill) -[:REQUIRES_CONCEPT]-> (Concept) : 147 items\n",
    "   ðŸ”¸ (Tool) -[:BUILT_WITH]-> (ProgrammingLanguage) : 60 items\n",
    "   ðŸ”¸ (Platform) -[:BUILT_WITH]-> (ProgrammingLanguage) : 2 items\n",
    "   ðŸ”¸ (Platform) -[:IS_SIMILAR_TO]-> (Platform) : 53 items\n",
    "   ðŸ”¸ (Concept) -[:IS_SIMILAR_TO]-> (Concept) : 86 items\n",
    "   ðŸ”¸ (Concept) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 455 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:USES_LANGUAGE]-> (ProgrammingLanguage) : 16 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:IS_SIMILAR_TO]-> (ProgrammingLanguage) : 54 items\n",
    "   ðŸ”¸ (Concept) -[:null]-> (null) : 12 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:IS_SIMILAR_TO]-> (Tool) : 10 items\n",
    "   ðŸ”¸ (Tool) -[:IS_SIMILAR_TO]-> (ProgrammingLanguage) : 6 items\n",
    "   ðŸ”¸ (Tool) -[:IS_SIMILAR_TO]-> (Platform) : 1 items\n",
    "   ðŸ”¸ (Company) -[:LOCATED_IN]-> (Location) : 456 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:RELATED_ROLE]-> (ProgrammingLanguage) : 9 items\n",
    "   ðŸ”¸ (Platform) -[:REQUIRES_TOOL]-> (ProgrammingLanguage) : 2 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_TOOL]-> (Tool) : 39 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_TOOL]-> (Platform) : 56 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_LANGUAGE]-> (ProgrammingLanguage) : 18 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:REQUIRES_TOOL]-> (Tool) : 11 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:REQUIRES_TOOL]-> (Platform) : 16 items\n",
    "   ðŸ”¸ (Company) -[:CREATED_BY]-> (Tool) : 7 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:REQUIRES_LANGUAGE]-> (ProgrammingLanguage) : 1 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_LANGUAGE]-> (ProgrammingLanguage) : 27 items\n",
    "   ðŸ”¸ (Company) -[:CREATED_BY]-> (ProgrammingLanguage) : 2 items\n",
    "   ðŸ”¸ (Company) -[:CREATED_BY]-> (Framework) : 3 items\n",
    "   ðŸ”¸ (Platform) -[:REQUIRES_LANGUAGE]-> (ProgrammingLanguage) : 9 items\n",
    "   ðŸ”¸ (Concept) -[:CREATED_BY]-> (Company) : 20 items\n",
    "   ðŸ”¸ (Concept) -[:REQUIRES_SKILL]-> (Skill) : 13 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_SKILL]-> (Skill) : 9 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_TOOL]-> (Framework) : 15 items\n",
    "   ðŸ”¸ (Platform) -[:REQUIRES_CONCEPT]-> (Concept) : 15 items\n",
    "   ðŸ”¸ (Concept) -[:REQUIRES_CONCEPT]-> (Concept) : 118 items\n",
    "   ðŸ”¸ (Company) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 47 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_CONCEPT]-> (Concept) : 49 items\n",
    "   ðŸ”¸ (Company) -[:REQUIRES_TOOL]-> (Tool) : 8 items\n",
    "   ðŸ”¸ (Company) -[:REQUIRES_CONCEPT]-> (Concept) : 1 items\n",
    "   ðŸ”¸ (Company) -[:REQUIRES_TOOL]-> (Platform) : 2 items\n",
    "   ðŸ”¸ (Tool) -[:IS_SIMILAR_TO]-> (Framework) : 3 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_TOOL]-> (Tool) : 67 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_CONCEPT]-> (Concept) : 55 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_TOOL]-> (Platform) : 13 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_TOOL]-> (Framework) : 42 items\n",
    "   ðŸ”¸ (Framework) -[:USES_TOOL]-> (Tool) : 5 items\n",
    "   ðŸ”¸ (Location) -[:LOCATED_IN]-> (Location) : 61 items\n",
    "   ðŸ”¸ (CloudService) -[:REQUIRES_TOOL]-> (CloudService) : 18 items\n",
    "   ðŸ”¸ (CloudService) -[:REQUIRES_LANGUAGE]-> (ProgrammingLanguage) : 22 items\n",
    "   ðŸ”¸ (CloudService) -[:REQUIRES_TOOL]-> (Tool) : 18 items\n",
    "   ðŸ”¸ (CloudService) -[:REQUIRES_TOOL]-> (Framework) : 3 items\n",
    "   ðŸ”¸ (Concept) -[:USES_LANGUAGE]-> (ProgrammingLanguage) : 4 items\n",
    "   ðŸ”¸ (Concept) -[:REQUIRES_TOOL]-> (Tool) : 2 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_TOOL]-> (Database) : 9 items\n",
    "   ðŸ”¸ (Framework) -[:RELATED_ROLE]-> (Framework) : 2 items\n",
    "   ðŸ”¸ (Framework) -[:RELATED_ROLE]-> (JobRole) : 2 items\n",
    "   ðŸ”¸ (Platform) -[:USES_LANGUAGE]-> (ProgrammingLanguage) : 47 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_PLATFORM]-> (Platform) : 4 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_FRAMEWORK]-> (Framework) : 2 items\n",
    "   ðŸ”¸ (Company) -[:CREATED_BY]-> (Company) : 14 items\n",
    "   ðŸ”¸ (Company) -[:REQUIRES_FRAMEWORK]-> (Framework) : 1 items\n",
    "   ðŸ”¸ (Company) -[:REQUIRES_TOOL]-> (Framework) : 2 items\n",
    "   ðŸ”¸ (Tool) -[:USES_LANGUAGE]-> (Concept) : 6 items\n",
    "   ðŸ”¸ (Framework) -[:USES_LANGUAGE]-> (Concept) : 1 items\n",
    "   ðŸ”¸ (Platform) -[:USES_LANGUAGE]-> (Concept) : 1 items\n",
    "   ðŸ”¸ (Framework) -[:IS_SIMILAR_TO]-> (Tool) : 3 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_TOOL]-> (CloudService) : 10 items\n",
    "   ðŸ”¸ (Framework) -[:BUILT_WITH]-> (ProgrammingLanguage) : 6 items\n",
    "   ðŸ”¸ (CloudService) -[:USES_LANGUAGE]-> (ProgrammingLanguage) : 12 items\n",
    "   ðŸ”¸ (Platform) -[:REQUIRES_TOOL]-> (Tool) : 9 items\n",
    "   ðŸ”¸ (Company) -[:CREATED_BY]-> (Platform) : 8 items\n",
    "   ðŸ”¸ (Company) -[:CREATED_BY]-> (Database) : 2 items\n",
    "   ðŸ”¸ (ProgrammingLanguage) -[:IS_SIMILAR_TO]-> (Framework) : 1 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_TOOL]-> (Database) : 17 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_CONCEPT]-> (Framework) : 1 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_CONCEPT]-> (Platform) : 1 items\n",
    "   ðŸ”¸ (Framework) -[:REQUIRES_SKILL]-> (Skill) : 23 items\n",
    "   ðŸ”¸ (Framework) -[:IS_SIMILAR_TO]-> (ProgrammingLanguage) : 1 items\n",
    "   ðŸ”¸ (Company) -[:CREATED_BY]-> (Concept) : 1 items\n",
    "   ðŸ”¸ (Concept) -[:IS_SIMILAR_TO]-> (Platform) : 4 items\n",
    "   ðŸ”¸ (Concept) -[:REQUIRES_TOOL]-> (Concept) : 1 items\n",
    "   ðŸ”¸ (CloudService) -[:LOCATED_IN]-> (Location) : 11 items\n",
    "   ðŸ”¸ (CloudService) -[:CREATED_BY]-> (CloudService) : 1 items\n",
    "   ðŸ”¸ (CloudService) -[:REQUIRES_TOOL]-> (Database) : 5 items\n",
    "   ðŸ”¸ (CloudService) -[:REQUIRES_CONCEPT]-> (Concept) : 18 items\n",
    "   ðŸ”¸ (Tool) -[:CREATED_BY]-> (CloudService) : 3 items\n",
    "   ðŸ”¸ (CloudService) -[:REQUIRES_TOOL]-> (Platform) : 4 items\n",
    "   ðŸ”¸ (Platform) -[:REQUIRES_TOOL]-> (Database) : 1 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_CLOUD_SERVICE]-> (CloudService) : 2 items\n",
    "   ðŸ”¸ (Platform) -[:REQUIRES_TOOL]-> (Platform) : 1 items\n",
    "   ðŸ”¸ (Location) -[:REQUIRES_CONCEPT]-> (Concept) : 4 items\n",
    "   ðŸ”¸ (Concept) -[:REQUIRES_TOOL]-> (Platform) : 1 items\n",
    "   ðŸ”¸ (Concept) -[:RELATED_ROLE]-> (Concept) : 15 items\n",
    "   ðŸ”¸ (Concept) -[:RELATED_ROLE]-> (Framework) : 1 items\n",
    "   ðŸ”¸ (Tool) -[:BUILT_WITH]-> (Tool) : 1 items\n",
    "   ðŸ”¸ (Platform) -[:REQUIRES_TOOL]-> (CloudService) : 2 items\n",
    "   ðŸ”¸ (Tool) -[:REQUIRES_TOOL]-> (CloudService) : 1 items\n",
    "   ðŸ”¸ (JobPosting) -[:IS_FOR_ROLE]-> (JobRole) : 640 items\n",
    "   ðŸ”¸ (JobPosting) -[:POSTED_BY]-> (Company) : 634 items\n",
    "   ðŸ”¸ (Database) -[:USES_LANGUAGE]-> (ProgrammingLanguage) : 7 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_CONCEPT]-> (ProgrammingLanguage) : 1 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_CLOUD]-> (CloudService) : 13 items\n",
    "   ðŸ”¸ (Skill) -[:IMPLEMENTS_CONCEPT]-> (Concept) : 237 items\n",
    "   ðŸ”¸ (Database) -[:BUILT_WITH]-> (ProgrammingLanguage) : 2 items\n",
    "   ðŸ”¸ (Tool) -[:USES_TOOL]-> (Tool) : 14 items\n",
    "   ðŸ”¸ (Platform) -[:USES_LANGUAGE]-> (Tool) : 1 items\n",
    "   ðŸ”¸ (Platform) -[:USES_TOOL]-> (Tool) : 1 items\n",
    "   ðŸ”¸ (Tool) -[:IS_SIMILAR_TO]-> (Concept) : 7 items\n",
    "   ðŸ”¸ (Tool) -[:BUILT_WITH]-> (Framework) : 1 items\n",
    "   ðŸ”¸ (JobRole) -[:REQUIRES_TOOL]-> (ProgrammingLanguage) : 2 items\n",
    "   ðŸ”¸ (Skill) -[:IMPLEMENTS_CONCEPT]-> (ProgrammingLanguage) : 2 items\n",
    "\n",
    "ðŸŽ‰ Ingestion Pipeline Complete!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3T-M7eJAcreV",
    "outputId": "1919ad10-8f4e-48fd-c7bb-07f0cbb21bad"
   },
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    NEO4J_URI = \"neo4j+s://0c60999b.databases.neo4j.io\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD = \"6Cp_0O2fK3elKHiYIqQsCyo_LiTPQNzobLW_Xw5Z70k\"\n",
    "    DB_NAME = \"neo4j\" # Standard for AuraDB\n",
    "\n",
    "# ==========================================\n",
    "# 2. TEST ENGINE\n",
    "# ==========================================\n",
    "def run_test_query(driver, test_name, cypher_query, params=None):\n",
    "    print(f\"\\nðŸ”¬ RUNNING TEST: {test_name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        # The modern API calls specific to your request\n",
    "        records, summary, keys = driver.execute_query(\n",
    "            cypher_query,\n",
    "            parameters_=params,  # Use named parameters if needed\n",
    "            database_=Config.DB_NAME\n",
    "        )\n",
    "\n",
    "        # 1. Print Performance Metrics\n",
    "        print(f\"âœ… Query executed in {summary.result_available_after} ms\")\n",
    "        print(f\"ðŸ“Š Records returned: {len(records)}\")\n",
    "\n",
    "        # 2. Pretty Print Results (Head 5)\n",
    "        if records:\n",
    "            print(\"\\n--- SAMPLE DATA ---\")\n",
    "            # Convert to Pandas DataFrame for cleaner visualization if you have it,\n",
    "            # otherwise just print the dicts\n",
    "            data = [r.data() for r in records[:5]]\n",
    "            for row in data:\n",
    "                print(row)\n",
    "        else:\n",
    "            print(\"âš ï¸ No records found (Check your match logic).\")\n",
    "\n",
    "        return records\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ TEST FAILED: {e}\")\n",
    "        return []\n",
    "\n",
    "# ==========================================\n",
    "# 3. THE TESTS\n",
    "# ==========================================\n",
    "def main():\n",
    "    with GraphDatabase.driver(Config.NEO4J_URI, auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD)) as driver:\n",
    "        driver.verify_connectivity()\n",
    "        print(\"ðŸ”Œ Connected. Starting Verification Suite...\")\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # TEST 1: The \"Reasoning Chain\" (Role -> Skill -> Concept)\n",
    "        # Goal: Verify we didn't just dump keywords, but actually linked HOW to WHY.\n",
    "        # ---------------------------------------------------------\n",
    "        query_chain = \"\"\"\n",
    "        MATCH path = (role:JobRole)-[:REQUIRES_SKILL]->(skill:Skill)-[:REQUIRES_CONCEPT|IMPLEMENTS_CONCEPT]->(concept:Concept)\n",
    "        WHERE role.name CONTAINS \"Engineer\" OR role.name CONTAINS \"Data\"\n",
    "        RETURN\n",
    "            role.name AS Role,\n",
    "            skill.name AS Required_Skill,\n",
    "            concept.name AS Underlying_Concept\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        run_test_query(driver, \"Reasoning Chain Check\", query_chain)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # TEST 2: The \"Tool Stack\" Analysis\n",
    "        # Goal: Ensure tools are categorized (Framework vs Library vs Database)\n",
    "        # ---------------------------------------------------------\n",
    "        query_tools = \"\"\"\n",
    "        MATCH (role:JobRole)-[:REQUIRES_TOOL|REQUIRES_FRAMEWORK|REQUIRES_DATABASE]->(tech)\n",
    "        RETURN\n",
    "            labels(tech)[0] AS Tech_Type,\n",
    "            count(*) AS Frequency,\n",
    "            collect(tech.name)[0..3] AS Examples\n",
    "        ORDER BY Frequency DESC\n",
    "        \"\"\"\n",
    "        run_test_query(driver, \"Tech Stack Categorization\", query_tools)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # TEST 3: The \"Orphan\" Check (Data Quality)\n",
    "        # Goal: Find nodes that failed to link to anything.\n",
    "        # (Low numbers are good. High numbers mean extraction failed).\n",
    "        # ---------------------------------------------------------\n",
    "        query_orphans = \"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE NOT (n)--()\n",
    "        RETURN labels(n)[0] AS Entity_Type, count(*) AS Orphan_Count\n",
    "        ORDER BY Orphan_Count DESC\n",
    "        \"\"\"\n",
    "        run_test_query(driver, \"Orphan Node Detection\", query_orphans)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TES7lRWmz96"
   },
   "source": [
    "HANDEL FAILED INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8DoKeGBcrhU",
    "outputId": "e65f6bc2-c580-44ea-de20-34ffce94eea9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def audit_failed_ingestion(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ Failure log not found at: {file_path}\")\n",
    "        return\n",
    "\n",
    "    total_failed_triplets = 0\n",
    "    total_failed_nodes = 0\n",
    "\n",
    "    print(f\"ðŸ” Auditing Failure Log: {os.path.basename(file_path)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # 1. Extract JSON arrays from UNWIND statements using Regex\n",
    "    # This finds everything between UNWIND [ and ] AS\n",
    "    batches = re.findall(r'UNWIND\\s+(\\[.*?\\])\\s+AS', content, re.DOTALL)\n",
    "\n",
    "    for i, batch_str in enumerate(batches):\n",
    "        try:\n",
    "            batch_data = json.loads(batch_str)\n",
    "            count = len(batch_data)\n",
    "\n",
    "            # 2. Distinguish between Node batches and Edge batches\n",
    "            # Node batches are simple lists of strings: [\"Python\", \"Java\"]\n",
    "            # Edge batches are lists of dicts: [{\"s\": \"...\", \"o\": \"...\"}]\n",
    "            if count > 0:\n",
    "                if isinstance(batch_data[0], dict):\n",
    "                    total_failed_triplets += count\n",
    "                else:\n",
    "                    total_failed_nodes += count\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"âš ï¸ Warning: Could not parse batch {i+1} - check for manual file corruption.\")\n",
    "\n",
    "    print(f\"ðŸ“Š AUDIT RESULTS:\")\n",
    "    print(f\"   âŒ Failed Nodes:         {total_failed_nodes}\")\n",
    "    print(f\"   âŒ Failed Relationships:  {total_failed_triplets}\")\n",
    "    print(f\"   ðŸ“‰ Total Missing Items:  {total_failed_nodes + total_failed_triplets}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if total_failed_triplets == 0 and total_failed_nodes == 0:\n",
    "        print(\"ðŸŽ‰ The log is empty or contains no valid Cypher blocks. You are likely 100% synced!\")\n",
    "\n",
    "# Run the audit\n",
    "FAILURE_LOG_PATH = \"/content/drive/MyDrive/rr_mlops/failed_ingestion_log.cypher\"\n",
    "audit_failed_ingestion(FAILURE_LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-zMbAHDSa92"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnoKP4m1SbAe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k_XxlVeSbC0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K02VtKvqSbpT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g53ju1CLBOK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdTds4xLLBRW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2voASURLLBUs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XABM2XhLBYI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZBHD-3YLBa-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLgk_Ml1cruS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06pFOJvIMKh7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWMKQMEaMKi5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQCi4lSoMKjx",
    "outputId": "7c5f4a73-5cd8-48ba-9cbc-a357644132a7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from google.colab import userdata\n",
    "\n",
    "api = userdata.get('groq-60days')\n",
    "\n",
    "client = Groq(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=api,\n",
    ")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SfkDl3UMKkw",
    "outputId": "e3975568-1801-4e0a-a21e-14abeea9b945"
   },
   "outputs": [],
   "source": [
    "def count_total_batches_v2():\n",
    "    print(\"ðŸ§ Calculating total batches (80k limit + 1.5k word Wiki cap)...\")\n",
    "\n",
    "    # 1. Initialize Generators with the NEW logic\n",
    "    role_gen = DataIngestor.read_roles(Config.PATH_ROLES)\n",
    "    job_gen = DataIngestor.read_jobs_csv(Config.PATH_JOBS)\n",
    "\n",
    "    # Ensure this matches the 1.5k word cap logic in your DataIngestor\n",
    "    wiki_gen = DataIngestor.read_wiki(Config.PATH_WIKI)\n",
    "\n",
    "    # 2. Initialize Batcher with NEW 80,000 character limit\n",
    "    all_batches = batch_generator(\n",
    "        [role_gen, job_gen, wiki_gen],\n",
    "        max_chars=53500\n",
    "    )\n",
    "\n",
    "    total_count = 0\n",
    "\n",
    "    # 3. Exhaust the generator (locally, no API cost)\n",
    "    for _ in all_batches:\n",
    "        total_count += 1\n",
    "\n",
    "    # 4. Results calculation\n",
    "    # Since you processed 33 batches at 20k, we calculate remaining work\n",
    "    # based on the new total at 80k.\n",
    "    # Note: 'processed' is reset because the batch definition changed.\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"ðŸ“Š NEW TOTAL BATCHES (80k): {total_count}\")\n",
    "    print(f\"â³ ESTIMATED REMAINING:      ~{total_count} batches\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    if total_count > 0:\n",
    "        # Larger batches take longer to process (usually 20-30s including rate limit padding)\n",
    "        est_seconds_per_batch = 25\n",
    "        est_minutes = (total_count * est_seconds_per_batch) / 60\n",
    "        print(f\"ðŸ•’ Estimated time to finish ALL: ~{round(est_minutes, 1)} minutes.\")\n",
    "        print(f\"ðŸš€ Speed Improvement: ~{round(388/total_count, 1)}x faster than 20k batches.\")\n",
    "\n",
    "# Execute the test\n",
    "count_total_batches_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmrxYfRR3SB8",
    "outputId": "56395e63-86c0-458c-f66f-4895d8b34a33"
   },
   "outputs": [],
   "source": [
    "def check_source_statistics():\n",
    "    print(\"ðŸ“Š Calculating word counts per source...\")\n",
    "\n",
    "    # Initialize Generators\n",
    "    sources = {\n",
    "        \"Roles (JSON)\": DataIngestor.read_roles(Config.PATH_ROLES),\n",
    "        \"Jobs (CSV)\": DataIngestor.read_jobs_csv(Config.PATH_JOBS),\n",
    "        \"Wiki (JSONL)\": DataIngestor.read_wiki(Config.PATH_WIKI)\n",
    "    }\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    for name, gen in sources.items():\n",
    "        word_count = 0\n",
    "        chunk_count = 0\n",
    "        for text in gen:\n",
    "            # Simple space-based word count\n",
    "            word_count += len(text.split())\n",
    "            chunk_count += 1\n",
    "        stats[name] = {\"words\": word_count, \"chunks\": chunk_count}\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"{'Source':<15} | {'Chunks':<10} | {'Word Count':<15}\")\n",
    "    print(\"-\" * 40)\n",
    "    total_words = 0\n",
    "    for name, data in stats.items():\n",
    "        print(f\"{name:<15} | {data['chunks']:<10} | {data['words']:<15,}\")\n",
    "        total_words += data['words']\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'TOTAL':<15} | {'':<10} | {total_words:<15,}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "check_source_statistics()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
