{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzjP4r_AYnNg"
   },
   "outputs": [],
   "source": [
    "!pip install -q google-genai pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G5jD7ToZO7_"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "api_key = userdata.get('gemini_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCJvGSYOdIHG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brLpSJJYdP6t"
   },
   "source": [
    "GENERATING WIKI ARTICLES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jNTL4wC8ZPDl",
    "outputId": "e10a9ffe-7fb5-409a-ce3c-b2eb6e5ea6fc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "BASE_DIR = \"/content/drive/MyDrive/rr_mlops/leftover_batches\"\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    os.makedirs(BASE_DIR)\n",
    "\n",
    "OUTPUT_FILE = os.path.join(BASE_DIR, \"wiki_source.jsonl\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. THE CORE KNOWLEDGE SYLLABUS\n",
    "# ==========================================\n",
    "TOPICS = [\n",
    "    # --- 1. THE \"MISSING\" BASICS (Languages & Runtimes) ---\n",
    "    \"Python (programming language)\", \"C++\", \"Java (programming language)\",\n",
    "    \"JavaScript\", \"TypeScript\", \"Go (programming language)\", \"Rust (programming language)\",\n",
    "    \"SQL\", \"Bash (Unix shell)\", \"Node.js\", \"Java Virtual Machine\", \"Kotlin\", \"Swift (programming language)\",\n",
    "    \"C#\", \"Ruby (programming language)\", \"Scala (programming language)\", \"Solidity\", \"Dart (programming language)\",\n",
    "\n",
    "    # --- 2. CLASSICAL ML ALGORITHMS ---\n",
    "    \"Random forest\", \"Gradient boosting\", \"XGBoost\", \"Decision tree learning\",\n",
    "    \"Support vector machine\", \"K-nearest neighbors algorithm\", \"Naive Bayes classifier\",\n",
    "    \"Linear regression\", \"Logistic regression\", \"Principal component analysis\",\n",
    "    \"K-means clustering\", \"DBSCAN\", \"Ensemble learning\", \"Dimensionality reduction\",\n",
    "    \"Time series analysis\", \"Bayesian statistics\", \"Causal inference\",\n",
    "\n",
    "    # --- 3. DEEP LEARNING & AI ---\n",
    "    \"Neural network\", \"Convolutional neural network\", \"Recurrent neural network\",\n",
    "    \"Transformer (machine learning)\", \"Attention (machine learning)\", \"Backpropagation\",\n",
    "    \"Large language model\", \"Generative pre-trained transformer\", \"BERT (language model)\",\n",
    "    \"Fine-tuning (machine learning)\", \"Transfer learning\", \"Embedding\",\n",
    "    \"Generative adversarial network\", \"Diffusion model\", \"Reinforcement learning\",\n",
    "    \"Computer vision\", \"Natural language processing\", \"Explainable AI\",\n",
    "\n",
    "    # --- 4. DATA STRUCTURES ---\n",
    "    \"Array (data structure)\", \"Linked list\", \"Hash table\", \"Binary search tree\",\n",
    "    \"Red‚Äìblack tree\", \"AVL tree\", \"B-tree\", \"Heap (data structure)\", \"Trie\",\n",
    "    \"Graph (abstract data type)\", \"Stack (abstract data type)\", \"Queue (abstract data type)\",\n",
    "    \"Bloom filter\", \"Disjoint-set data structure\",\n",
    "\n",
    "    # --- 5. ALGORITHMS ---\n",
    "    \"Sorting algorithm\", \"Quicksort\", \"Merge sort\", \"Binary search algorithm\",\n",
    "    \"Dynamic programming\", \"Dijkstra's algorithm\", \"A* search algorithm\",\n",
    "    \"Depth-first search\", \"Breadth-first search\", \"Greedy algorithm\",\n",
    "    \"Pathfinding\", \"Simultaneous localization and mapping\",\n",
    "\n",
    "    # --- 6. SYSTEM DESIGN & ARCHITECTURE ---\n",
    "    \"Microservices\", \"Monolithic application\", \"Load balancing (computing)\",\n",
    "    \"Database sharding\", \"Caching (computing)\", \"Content delivery network\",\n",
    "    \"CAP theorem\", \"ACID\", \"Consistent hashing\",\n",
    "    \"Replication (computing)\", \"Leader election\", \"Distributed system\",\n",
    "    \"Event-driven architecture\", \"REST\", \"GraphQL\", \"gRPC\", \"WebSocket\",\n",
    "    \"Serverless computing\", \"Service mesh\", \"API gateway\", \"OAuth\", \"JWT\",\n",
    "\n",
    "    # --- 7. MLOps & DEPLOYMENT ---\n",
    "    \"CI/CD\", \"DevOps\", \"MLOps\", \"Containerization (computing)\", \"Kubernetes\",\n",
    "    \"Docker (software)\", \"Infrastructure as code\", \"Terraform (software)\",\n",
    "    \"Model serving\", \"Feature store\", \"Data lineage\", \"A/B testing\",\n",
    "    \"Model monitoring\", \"Experiment tracking\", \"Data version control\",\n",
    "\n",
    "    # --- 8. COMMON TOOLS & FRAMEWORKS ---\n",
    "    \"Librosa\", \"Streamlit\", \"Flask (web framework)\", \"Django (web framework)\",\n",
    "    \"FastAPI\", \"Spring Boot\", \"React (software)\", \"Angular (web framework)\",\n",
    "    \"Apache Kafka\", \"RabbitMQ\", \"Redis\", \"PostgreSQL\", \"MongoDB\", \"Elasticsearch\",\n",
    "    \"Apache Spark\", \"Hadoop\", \"Pandas (software)\", \"NumPy\", \"Scikit-learn\",\n",
    "\n",
    "    # ==============================================================================\n",
    "    # NEW EXPANSIONS BASED ON JOB DESCRIPTIONS\n",
    "    # ==============================================================================\n",
    "\n",
    "    # --- 9. GENERATIVE AI & LLM STACK ---\n",
    "    \"Retrieval-augmented generation\", \"LangChain\", \"LlamaIndex\", \"Vector database\",\n",
    "    \"Pinecone (database)\", \"Weaviate\", \"Prompt engineering\", \"Parameter-efficient fine-tuning\",\n",
    "    \"Low-rank adaptation\", \"Hugging Face Transformers\", \"Stable Diffusion\", \"Synthetic data\",\n",
    "    \"Hallucination (artificial intelligence)\", \"Model alignment\",\n",
    "\n",
    "    # --- 10. MODERN DATA ENGINEERING & ANALYTICS ---\n",
    "    \"Data warehouse\", \"Data lake\", \"Data mesh\", \"Snowflake (software)\", \"Google BigQuery\",\n",
    "    \"Amazon Redshift\", \"Databricks\", \"Apache Airflow\", \"dbt (software)\", \"ETL\", \"ELT\",\n",
    "    \"Apache Flink\", \"Star schema\", \"Slowly changing dimension\", \"Data governance\",\n",
    "\n",
    "    # --- 11. CLOUD NATIVE & OBSERVABILITY ---\n",
    "    \"Amazon Web Services\", \"Microsoft Azure\", \"Google Cloud Platform\", \"Helm (software)\",\n",
    "    \"Argo CD\", \"Prometheus (software)\", \"Grafana\", \"OpenTelemetry\", \"Istio\",\n",
    "    \"Chaos engineering\", \"Site reliability engineering\", \"Infrastructure as Service\",\n",
    "    \"Platform as a Service\", \"GitOps\",\n",
    "\n",
    "    # --- 12. FRONTEND & MOBILE DEVELOPMENT ---\n",
    "    \"Vue.js\", \"Next.js\", \"Tailwind CSS\", \"Redux (JavaScript library)\", \"Webpack\",\n",
    "    \"React Native\", \"Flutter (software)\", \"SwiftUI\", \"Jetpack Compose\", \"Android Studio\",\n",
    "    \"Xcode\", \"Progressive web app\", \"Single-page application\", \"WebAssembly\",\n",
    "\n",
    "    # --- 13. GAME DEV & GRAPHICS ---\n",
    "    \"Unity (game engine)\", \"Unreal Engine\", \"OpenGL\", \"Vulkan (API)\", \"DirectX\",\n",
    "    \"Shader\", \"Ray tracing (graphics)\", \"3D computer graphics\", \"Physics engine\",\n",
    "    \"Augmented reality\", \"Virtual reality\", \"WebGL\",\n",
    "\n",
    "    # --- 14. SECURITY & BLOCKCHAIN ---\n",
    "    \"Cybersecurity\", \"Penetration test\", \"Identity management\", \"Zero trust security model\",\n",
    "    \"OWASP\", \"Smart contract\", \"Ethereum\", \"Decentralized application\", \"Web3\",\n",
    "    \"Cryptography\", \"Public-key cryptography\", \"Consensus algorithm\",\n",
    "\n",
    "    # --- 15. EMBEDDED & HARDWARE ---\n",
    "    \"Embedded system\", \"Real-time operating system\", \"Microcontroller\", \"Firmware\",\n",
    "    \"Internet of things\", \"Field-programmable gate array\", \"System on a chip\",\n",
    "    \"Communication protocol\", \"I2C\", \"SPI\", \"UART\"\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 3. WIKI FETCHER ENGINE (FIXED)\n",
    "# ==========================================\n",
    "def fetch_wiki_content(title):\n",
    "    \"\"\"Fetches full text from Wikipedia API with User-Agent.\"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # üö® FIX: Wikipedia requires a User-Agent or it blocks you\n",
    "    headers = {\n",
    "        \"User-Agent\": \"CareerKnowledgeGraphBot/1.0 (educational_project@example.com)\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"redirects\": 1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "\n",
    "        # Check if the response is valid\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ö†Ô∏è HTTP Error {response.status_code} for {title}\")\n",
    "            return None\n",
    "\n",
    "        data = response.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "\n",
    "        for page_id, page_data in pages.items():\n",
    "            if page_id == \"-1\":\n",
    "                # Try simple search fallback if direct title lookup fails\n",
    "                print(f\"   Note: Direct lookup failed for '{title}', skipping...\")\n",
    "                return None\n",
    "            if \"extract\" in page_data:\n",
    "                return page_data[\"extract\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "# ==========================================\n",
    "# 4. EXECUTION\n",
    "# ==========================================\n",
    "print(f\"üöÄ Initializing Knowledge Injection for {len(TOPICS)} core topics...\")\n",
    "print(f\"üìÇ Output Target: {OUTPUT_FILE}\")\n",
    "\n",
    "success_count = 0\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    for topic in tqdm(TOPICS, desc=\"Fetching Wiki Articles\"):\n",
    "        content = fetch_wiki_content(topic)\n",
    "\n",
    "        if content:\n",
    "            entry = {\n",
    "                \"title\": topic,\n",
    "                \"text\": content,\n",
    "                \"source\": \"wikipedia_core_concepts\",\n",
    "                \"keyword\": \"Computer Science Fundamentals\"\n",
    "            }\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "            success_count += 1\n",
    "            time.sleep(0.5) # Polite rate limiting\n",
    "        else:\n",
    "            pass # We already logged the error\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"‚úÖ SUCCESSFULLY FETCHED: {success_count} / {len(TOPICS)} Articles\")\n",
    "print(f\"üìù Saved to: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AINh_FQmZPEn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQBocDexZPMr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wEsAOPWZPOM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ue_cLpyIZPPi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYLyOsrodlmz"
   },
   "source": [
    "CLEANING GENERATED CYPHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM7LJQ9-ZPQn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irq9LH9-ZPRi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432,
     "referenced_widgets": [
      "d2c0e85b12224ece99e3d01ae0a847d8",
      "d904276a4bf84414b819a114d976436c",
      "bfa86959562b4d3ea26b6aaf0a5e4d81",
      "2c5f1b712c0342769ff3cbcd60e4b549",
      "5fa52890953d4805a7cd4d0192897787",
      "65ce337c9370464ab14b89aa0ca08907",
      "34a018781ac24dc4acd99487c9f08c94",
      "69cb30a4a4b04ce794244ebae5fc6677",
      "57fdf46ebc3b49a695f3a4d1f7a54cd4",
      "f7297daa75ca4e83a1a5a816392481ae",
      "56dd9bdc565c4a09a58fa92e2df11784"
     ]
    },
    "id": "IbIwrlBSZPSZ",
    "outputId": "771bc5bf-ce54-4c6b-ea8c-2883ef96f465"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    API_KEY = api_key # <--- PASTE YOUR KEY HERE\n",
    "    MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "    # Paths (Pointing to the new LEFTOVER batch location)\n",
    "    BASE_DIR = \"/content/drive/MyDrive/rr_mlops/leftover_batches\"\n",
    "\n",
    "    # Inputs (We use the file we just generated)\n",
    "    PATH_WIKI = os.path.join(BASE_DIR, \"wiki_source.jsonl\")\n",
    "\n",
    "    # Batching\n",
    "    BATCH_SIZE_CHARS = 53500  # Increased for Wiki articles (Context rich)\n",
    "    RESUME_FROM_BATCH = 39\n",
    "\n",
    "    # Output (Saved directly to leftover folder)\n",
    "    # We write to a temp file first, then move to Drive to prevent partial corruption\n",
    "    LOCAL_OUTPUT_FILE = \"extracted_leftover_DAY6.jsonl\"\n",
    "    DRIVE_OUTPUT_PATH = os.path.join(BASE_DIR, LOCAL_OUTPUT_FILE)\n",
    "\n",
    "# ==========================================\n",
    "# 2. THE BRAIN (STRICT SYSTEM PROMPT)\n",
    "# ==========================================\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a Knowledge Graph Extraction Engine for a Career Advisory System.\n",
    "\n",
    "Your task is to extract structured knowledge that enables EXPLANATION, not just storage.\n",
    "\n",
    "This knowledge graph must support:\n",
    "1. WHY a job is the best match for a candidate\n",
    "2. WHY other roles are second or third best\n",
    "3. WHAT specific skills or concepts the candidate is missing\n",
    "4. HOW skills transfer across roles via shared concepts\n",
    "\n",
    "You must STRICTLY follow the ontology, rules, and output format below.\n",
    "If something does not fit the rules, DO NOT output it.\n",
    "\n",
    "--------------------------------------------------\n",
    "ONTOLOGY CONTRACT (STRICT)\n",
    "--------------------------------------------------\n",
    "\n",
    "ALLOWED NODE TYPES:\n",
    "JobPosting\n",
    "JobRole\n",
    "Skill\n",
    "ProgrammingLanguage\n",
    "Framework\n",
    "Tool\n",
    "Platform\n",
    "CloudService\n",
    "Database\n",
    "Concept\n",
    "Company\n",
    "Location\n",
    "\n",
    "--------------------------------------------------\n",
    "CORE SEMANTIC PRINCIPLE (MANDATORY)\n",
    "--------------------------------------------------\n",
    "\n",
    "The graph MUST separate:\n",
    "- EXECUTION (Skill)\n",
    "- EXPLANATION (Concept)\n",
    "\n",
    "This separation is NON-NEGOTIABLE.\n",
    "\n",
    "--------------------------------------------------\n",
    "SKILL vs CONCEPT CLASSIFICATION (CRITICAL RULE)\n",
    "--------------------------------------------------\n",
    "\n",
    "Skill = \"Can this be directly practiced, implemented, or demonstrated?\"\n",
    "\n",
    "Concept = \"Does this explain WHY or WHAT principle something is based on?\"\n",
    "\n",
    "HARD CONSTRAINT:\n",
    "If a Skill candidate can be rewritten as\n",
    "\"Understanding of X\" or \"Knowledge of X\",\n",
    "it MUST be classified as Concept.\n",
    "\n",
    "ANCHORING RULE:\n",
    "Every Skill emitted for a JobRole SHOULD be supported\n",
    "by at least one Concept via either:\n",
    "- JobRole REQUIRES_CONCEPT\n",
    "- Tool IMPLEMENTS_CONCEPT\n",
    "\n",
    "If no Concept exists add one according to your knowledge.\n",
    "If the current wiki article seems small as we are splitting the text across batch for current node make rekations according to your knowledge if there are not many made beacuse of less content\n",
    "\n",
    "VALID Skill examples:\n",
    "- SQL Query Writing\n",
    "- REST API Development\n",
    "- Feature Engineering\n",
    "- Model Deployment\n",
    "- Prompt Engineering\n",
    "- Kubernetes Troubleshooting\n",
    "- CI/CD Pipeline Implementation\n",
    "- Data Visualization\n",
    "- Unit Testing\n",
    "- API Integration\n",
    "\n",
    "INVALID as Skill (MUST be Concept):\n",
    "- Machine Learning\n",
    "- Deep Learning\n",
    "- Object-Oriented Programming\n",
    "- System Design\n",
    "- Cloud Architecture\n",
    "- Distributed Systems\n",
    "- Microservices Architecture\n",
    "- Natural Language Processing\n",
    "- Security Best Practices\n",
    "- Data Modeling\n",
    "- Statistics\n",
    "\n",
    "RULES:\n",
    "- If a term explains WHY ‚Üí Concept\n",
    "- If a term explains HOW ‚Üí Skill\n",
    "- If uncertain ‚Üí Concept\n",
    "- NEVER attach vague domains as Skill\n",
    "\n",
    "--------------------------------------------------\n",
    "TOOL vs FRAMEWORK vs PLATFORM (STRICT DEFINITIONS)\n",
    "--------------------------------------------------\n",
    "Classify technical entities using these rules:\n",
    "\n",
    "1. FRAMEWORK: \"Inverts control\". It calls your code. It provides a scaffold.\n",
    "   - EXAMPLES: React, Angular, Vue, Django, Spring Boot, TensorFlow, PyTorch, Flutter.\n",
    "   - RULE: If it dictates the architecture of the app, it is a Framework.\n",
    "\n",
    "2. TOOL: A library, utility, or software used to perform a task. You call it.\n",
    "   - EXAMPLES: Pandas (Library -> Tool), NumPy (Library -> Tool), Git (CLI Tool), Docker (Container Tool), Jenkins (Build Tool), Jira (Project Tool).\n",
    "   - RULE: If you import it to do math/data manipulation, it is a Tool (Library).\n",
    "\n",
    "3. PLATFORM: An environment where software runs or is hosted.\n",
    "   - EXAMPLES: iOS, Android, Linux, Windows, GitHub (SaaS Platform), GitLab.\n",
    "\n",
    "--------------------------------------------------\n",
    "ALLOWED RELATIONS (SEMANTIC)\n",
    "--------------------------------------------------\n",
    "\n",
    "-- Job Structure --\n",
    "POSTED_BY          (JobPosting -> Company)\n",
    "LOCATED_IN         (Company -> Location)\n",
    "IS_FOR_ROLE        (JobPosting -> JobRole)\n",
    "RELATED_ROLE       (JobRole -> JobRole)\n",
    "\n",
    "-- Role Requirements (EXPLANATION-FIRST) --\n",
    "REQUIRES_SKILL     (JobRole -> Skill)\n",
    "REQUIRES_LANGUAGE  (JobRole -> ProgrammingLanguage)\n",
    "REQUIRES_TOOL      (JobRole -> Tool | Framework | Database | Platform | CloudService)\n",
    "REQUIRES_CONCEPT   (JobRole -> Concept)\n",
    "\n",
    "-- Technical Reasoning (WHY / HOW CHAINS) --\n",
    "IMPLEMENTS_CONCEPT (Tool | Framework | ProgrammingLanguage -> Concept)\n",
    "USES_LANGUAGE      (Framework | Tool -> ProgrammingLanguage)\n",
    "BUILT_WITH         (Tool -> ProgrammingLanguage)\n",
    "IS_SIMILAR_TO      (Tool | Framework | Database -> same type)\n",
    "CREATED_BY         (ProgrammingLanguage | Framework | Tool -> Company)\n",
    "\n",
    "--------------------------------------------------\n",
    "ALLOWED TRIPLE PATTERNS (MANDATORY)\n",
    "--------------------------------------------------\n",
    "\n",
    "You may emit a triplet ONLY if it matches one of these exact patterns:\n",
    "\n",
    "-- Job Posting Context --\n",
    "(JobPosting) IS_FOR_ROLE (JobRole)\n",
    "(JobPosting) POSTED_BY (Company)\n",
    "(Company) LOCATED_IN (Location)\n",
    "\n",
    "-- Role Definition Context --\n",
    "(JobRole) REQUIRES_SKILL (Skill)\n",
    "(JobRole) REQUIRES_LANGUAGE (ProgrammingLanguage)\n",
    "(JobRole) REQUIRES_TOOL (Tool | Framework | Database | Platform | CloudService)\n",
    "(JobRole) REQUIRES_CONCEPT (Concept)\n",
    "(JobRole) RELATED_ROLE (JobRole)\n",
    "\n",
    "-- Technical Knowledge Context --\n",
    "(Framework | Tool | ProgrammingLanguage) IMPLEMENTS_CONCEPT (Concept)\n",
    "(Framework | Tool) USES_LANGUAGE (ProgrammingLanguage)\n",
    "(Tool | Framework | Database) IS_SIMILAR_TO (same type)\n",
    "(Tool) BUILT_WITH (ProgrammingLanguage)\n",
    "(ProgrammingLanguage | Framework | Tool) CREATED_BY (Company)\n",
    "\n",
    "If a candidate triplet does NOT match these patterns ‚Üí DISCARD IT.\n",
    "\n",
    "--------------------------------------------------\n",
    "FORBIDDEN OUTPUT (NEVER EMIT)\n",
    "--------------------------------------------------\n",
    "\n",
    "Do NOT output:\n",
    "- Responsibilities or actions\n",
    "- Soft skills (communication, leadership, teamwork)\n",
    "- Abstract fillers (systems, solutions, techniques)\n",
    "- Version numbers or editions\n",
    "- Salary, experience ranges, dates\n",
    "- Metrics or percentages\n",
    "- Multi-sentence nodes\n",
    "\n",
    "--------------------------------------------------\n",
    "SOURCE-AWARE RULES\n",
    "--------------------------------------------------\n",
    "\n",
    "IF INPUT SOURCE IS JOB DESCRIPTION (CSV):\n",
    "- Treat each record as a JobPosting.\n",
    "- ALWAYS map JobPosting ‚Üí JobRole using IS_FOR_ROLE.\n",
    "- Extract ONLY explicitly mentioned technical requirements.\n",
    "- Do NOT infer skills not written in text.\n",
    "\n",
    "IF INPUT SOURCE IS ROLE DEFINITION (JSON):\n",
    "- Subject is the JobRole.\n",
    "- Extract REQUIRED skills, tools, languages, and concepts.\n",
    "- Extract adjacent or alternative roles using RELATED_ROLE.\n",
    "\n",
    "IF INPUT SOURCE IS WIKI ARTICLE:\n",
    "- DO NOT emit JobPosting or JobRole.\n",
    "- Focus only on ProgrammingLanguage, Framework, Tool, Concept.\n",
    "- Emit Company ONLY if it is the creator (CREATED_BY).\n",
    "- Emit EXACTLY ONE definition object for the primary entity.\n",
    "\n",
    "--------------------------------------------------\n",
    "WIKI-SPECIFIC: DEFINITION EXTRACTION (MANDATORY)\n",
    "--------------------------------------------------\n",
    "\n",
    "If and ONLY if the input source is a Wiki article, return ONE definition object.\n",
    "\n",
    "Definition MUST focus on UTILITY for career understanding.\n",
    "\n",
    "Schema:\n",
    "- definition: ONE sentence explaining what it is and why it is useful.\n",
    "- summary: 2 sentences explaining where and why it is used.\n",
    "- key_characteristics: 3‚Äì5 short, technical bullet points.\n",
    "- source: provenance string (e.g. \"wikipedia\").\n",
    "\n",
    "Do NOT include history, dates, or trivia.\n",
    "\n",
    "--------------------------------------------------\n",
    "ENTITY NORMALIZATION RULES\n",
    "--------------------------------------------------\n",
    "\n",
    "- Canonical names only (JS ‚Üí JavaScript).\n",
    "- Singular form only.\n",
    "- Title Case for entities.\n",
    "- Disambiguate using context.\n",
    "- Prefer CloudService over Platform for AWS, Azure, GCP.\n",
    "\n",
    "--------------------------------------------------\n",
    "OUTPUT FORMAT (STRICT JSON ONLY)\n",
    "--------------------------------------------------\n",
    "\n",
    "Return ONLY a valid JSON object. No markdown. No explanations.\n",
    "\n",
    "{\n",
    "  \"triplets\": [\n",
    "    {\n",
    "      \"subject\": \"string\",\n",
    "      \"subject_type\": \"string\",\n",
    "      \"relation\": \"string\",\n",
    "      \"object\": \"string\",\n",
    "      \"object_type\": \"string\"\n",
    "    }\n",
    "  ],\n",
    "  \"definitions\": [\n",
    "    {\n",
    "      \"name\": \"string\",\n",
    "      \"node_type\": \"string\",\n",
    "      \"definition\": \"string\",\n",
    "      \"summary\": \"string\",\n",
    "      \"key_characteristics\": [\"string\"],\n",
    "      \"source\": \"string\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. DATA INGESTOR\n",
    "# ==========================================\n",
    "class DataIngestor:\n",
    "    @staticmethod\n",
    "    def read_wiki(path):\n",
    "        print(f\"Loading Wiki from: {path}\")\n",
    "        if not os.path.exists(path): return\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    title = item.get('title', 'Unknown')\n",
    "                    content = item.get('text', '')\n",
    "                    # Cap at 2500 words for detailed concepts\n",
    "                    words = content.split()\n",
    "                    capped_content = \" \".join(words[:2500])\n",
    "                    yield (\n",
    "                        f\"SOURCE: WIKI_ARTICLE\\n\"\n",
    "                        f\"PRIMARY_ENTITY: {title}\\n\"\n",
    "                        f\"CONTENT: {capped_content}\\n\\n\"\n",
    "                    )\n",
    "                except: continue\n",
    "\n",
    "def batch_generator(iterable, max_chars):\n",
    "    current_batch = []\n",
    "    current_len = 0\n",
    "    for text_chunk in iterable:\n",
    "        chunk_len = len(text_chunk)\n",
    "        if current_len + chunk_len > max_chars:\n",
    "            yield \"\".join(current_batch)\n",
    "            current_batch = []\n",
    "            current_len = 0\n",
    "        current_batch.append(text_chunk)\n",
    "        current_len += chunk_len\n",
    "    if current_batch: yield \"\".join(current_batch)\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "def main():\n",
    "    client = genai.Client(api_key=Config.API_KEY)\n",
    "\n",
    "    wiki_gen = DataIngestor.read_wiki(Config.PATH_WIKI)\n",
    "    all_batches = batch_generator(wiki_gen, Config.BATCH_SIZE_CHARS)\n",
    "\n",
    "    print(f\"\\nüöÄ Starting Knowledge Extraction (Batch Size: {Config.BATCH_SIZE_CHARS})...\")\n",
    "\n",
    "    # We append to the file so we don't overwrite if we restart\n",
    "    mode = 'a' if os.path.exists(Config.DRIVE_OUTPUT_PATH) else 'w'\n",
    "\n",
    "    with open(Config.DRIVE_OUTPUT_PATH, mode, encoding='utf-8') as f_out:\n",
    "        for i, batch_text in enumerate(tqdm(all_batches), 1):\n",
    "            if i < Config.RESUME_FROM_BATCH: continue\n",
    "\n",
    "            try:\n",
    "                response = client.models.generate_content(\n",
    "                    model=Config.MODEL_NAME,\n",
    "                    contents=f\"{SYSTEM_PROMPT}\\n\\nDATA TO EXTRACT:\\n{batch_text}\",\n",
    "                    config=types.GenerateContentConfig(response_mime_type=\"application/json\")\n",
    "                )\n",
    "\n",
    "                if response.text:\n",
    "                    # Sanity check: Ensure it's valid JSON before writing\n",
    "                    json.loads(response.text)\n",
    "\n",
    "                    f_out.write(response.text + \"\\n\")\n",
    "                    f_out.flush()\n",
    "                    print(f\"‚úÖ Batch {i} saved to Drive.\")\n",
    "\n",
    "                time.sleep(15) # Fast model, lower latency needed\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Batch {i} Error: {e}\")\n",
    "                time.sleep(30)\n",
    "\n",
    "    print(f\"\\nüéâ Extraction Complete! Saved to: {Config.DRIVE_OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc-MnHp9fNJ2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd2W960yfNat"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DqNUKlTraqcu",
    "outputId": "9ee20864-e778-4a0a-c36d-ff5b0e08a124"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "BASE_DIR = \"/content/drive/MyDrive/rr_mlops/leftover_batches\"\n",
    "INPUT_FILE = os.path.join(BASE_DIR, \"extracted_leftover_DAY6.jsonl\")\n",
    "CLEAN_FILE = os.path.join(BASE_DIR, \"clean_leftover_DAY6.jsonl\")\n",
    "CYPHER_FILE = os.path.join(BASE_DIR, \"ingest_leftover_DAY6.cypher\")\n",
    "\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# ==========================================\n",
    "# 2. ONTOLOGY (UNCHANGED)\n",
    "# ==========================================\n",
    "TYPE_OVERRIDES = {\n",
    "    # Data Science / ML\n",
    "    \"Pandas\": \"Tool\", \"NumPy\": \"Tool\", \"Scikit-Learn\": \"Framework\",\n",
    "    \"Keras\": \"Framework\", \"PyTorch\": \"Framework\", \"TensorFlow\": \"Framework\",\n",
    "    \"OpenCV\": \"Tool\", \"NLTK\": \"Tool\", \"SpaCy\": \"Tool\", # OpenCV/NLTK are libraries -> Tools\n",
    "    \"Matplotlib\": \"Tool\", \"Seaborn\": \"Tool\", \"D3.js\": \"Tool\", \"MLflow\": \"Tool\",\n",
    "    # DevOps / Cloud\n",
    "    \"Docker\": \"Tool\", \"Kubernetes\": \"Tool\", \"Terraform\": \"Tool\",\n",
    "    \"Jenkins\": \"Tool\", \"Git\": \"Tool\", \"Ansible\": \"Tool\",\n",
    "    \"AWS\": \"CloudService\", \"Azure\": \"CloudService\", \"GCP\": \"CloudService\",\n",
    "    # Concepts (Must NEVER be Skills)\n",
    "    \"Statistics\": \"Concept\", \"Probability\": \"Concept\", \"Linear Algebra\": \"Concept\",\n",
    "    \"Calculus\": \"Concept\", \"Machine Learning\": \"Concept\", \"Deep Learning\": \"Concept\",\n",
    "    \"Data Science\": \"Concept\", \"Computer Vision\": \"Concept\", \"NLP\": \"Concept\",\n",
    "    \"CI/CD\": \"Concept\", \"DevOps\": \"Concept\", \"MLOps\": \"Concept\",\n",
    "    \"Agile\": \"Concept\", \"Scrum\": \"Concept\", \"REST\": \"Concept\",\n",
    "    \"Microservices\": \"Concept\", \"System Design\": \"Concept\", \"Algorithms\": \"Concept\",\n",
    "    \"Data Structures\": \"Concept\", \"Object-Oriented Programming\": \"Concept\"\n",
    "}\n",
    "\n",
    "# B. Canonical Name Mapping (The \"Alias Crusher\")\n",
    "# Maps variations -> Single Official Name\n",
    "CANONICAL_NAMES = {\n",
    "    # JS Ecosystem\n",
    "    \"react.js\": \"React\", \"reactjs\": \"React\",\n",
    "    \"vue.js\": \"Vue\", \"vuejs\": \"Vue\", \"vue\": \"Vue\",\n",
    "    \"node.js\": \"Node.js\", \"nodejs\": \"Node.js\", \"node\": \"Node.js\",\n",
    "    \"express.js\": \"Express\", \"expressjs\": \"Express\",\n",
    "    \"next.js\": \"Next.js\", \"nextjs\": \"Next.js\",\n",
    "    # Cloud\n",
    "    \"amazon web services\": \"AWS\", \"aws\": \"AWS\",\n",
    "    \"google cloud platform\": \"GCP\", \"google cloud\": \"GCP\", \"gcp\": \"GCP\",\n",
    "    \"microsoft azure\": \"Azure\", \"azure\": \"Azure\",\n",
    "    # Databases\n",
    "    \"postgresql\": \"PostgreSQL\", \"postgres\": \"PostgreSQL\",\n",
    "    \"mongodb\": \"MongoDB\", \"mongo\": \"MongoDB\",\n",
    "    \"mssql\": \"SQL Server\", \"sql server\": \"SQL Server\",\n",
    "    # ML/Python\n",
    "    \"tensorflow\": \"TensorFlow\", \"tf\": \"TensorFlow\",\n",
    "    \"pytorch\": \"PyTorch\",\n",
    "    \"scikit-learn\": \"Scikit-Learn\", \"sklearn\": \"Scikit-Learn\",\n",
    "    \"mlflow\": \"MLflow\",\n",
    "    # General\n",
    "    \"golang\": \"Go\", \"go lang\": \"Go\",\n",
    "    \"c#\": \"C#\", \"c sharp\": \"C#\", \"c++\": \"C++\",\n",
    "    \"dotnet\": \".NET\", \".net\": \".NET\", \".net core\": \".NET Core\"\n",
    "}\n",
    "\n",
    "# C. Vague Phrase Detector\n",
    "BAD_PATTERNS = [\n",
    "    r\"\\bability to\\b\", r\"\\bexperience in\\b\", r\"\\bknowledge of\\b\",\n",
    "    r\"\\bresponsible for\\b\", r\"\\bworking with\\b\", r\"\\bunderstanding of\\b\",\n",
    "    r\"\\bfamiliarity with\\b\", r\"\\bproven track record\\b\", r\"\\bproficient in\\b\"\n",
    "]\n",
    "\n",
    "# D. Adjective Stripper\n",
    "ADJECTIVES = [\n",
    "    r\"^Strong\\s+\", r\"^Advanced\\s+\", r\"^Basic\\s+\", r\"^Senior\\s+\", r\"^Junior\\s+\",\n",
    "    r\"^Hands-on\\s+\", r\"^Expertise in\\s+\", r\"\\s+Skills$\", r\"\\s+Development$\"\n",
    "]\n",
    "\n",
    "# E. Knowledge Injection\n",
    "INJECTED_EDGES = {\n",
    "    \"PyTorch\": [(\"IMPLEMENTS_CONCEPT\", \"Deep Learning\", \"Concept\")],\n",
    "    \"TensorFlow\": [(\"IMPLEMENTS_CONCEPT\", \"Deep Learning\", \"Concept\")],\n",
    "    \"Scikit-Learn\": [(\"IMPLEMENTS_CONCEPT\", \"Machine Learning\", \"Concept\")],\n",
    "    \"React\": [(\"IMPLEMENTS_CONCEPT\", \"Frontend Development\", \"Concept\"), (\"IMPLEMENTS_CONCEPT\", \"Component-Based Architecture\", \"Concept\")],\n",
    "    \"Docker\": [(\"IMPLEMENTS_CONCEPT\", \"Containerization\", \"Concept\")],\n",
    "    \"Kubernetes\": [(\"IMPLEMENTS_CONCEPT\", \"Orchestration\", \"Concept\")],\n",
    "    \"PostgreSQL\": [(\"IMPLEMENTS_CONCEPT\", \"Relational Database\", \"Concept\")],\n",
    "    \"MongoDB\": [(\"IMPLEMENTS_CONCEPT\", \"NoSQL\", \"Concept\")],\n",
    "    \"REST\": [(\"IS_SUBTOPIC_OF\", \"Architectural Style\", \"Concept\")],\n",
    "    \"CI/CD\": [(\"IS_SUBTOPIC_OF\", \"DevOps Practice\", \"Concept\")]\n",
    "}\n",
    "\n",
    "# F. Allowed Pattern Schema\n",
    "ALLOWED_PATTERNS = {\n",
    "    (\"JobRole\", \"REQUIRES_SKILL\", \"Skill\"),\n",
    "    (\"JobRole\", \"REQUIRES_KNOWLEDGE\", \"Concept\"),\n",
    "    (\"JobRole\", \"REQUIRES_CONCEPT\", \"Concept\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"Tool\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"Framework\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"Database\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"Platform\"),\n",
    "    (\"JobRole\", \"REQUIRES_TOOL\", \"CloudService\"),\n",
    "    (\"JobRole\", \"REQUIRES_LANGUAGE\", \"ProgrammingLanguage\"),\n",
    "    (\"Tool\", \"IMPLEMENTS_CONCEPT\", \"Concept\"),\n",
    "    (\"Framework\", \"IMPLEMENTS_CONCEPT\", \"Concept\"),\n",
    "    (\"ProgrammingLanguage\", \"IMPLEMENTS_CONCEPT\", \"Concept\"),\n",
    "    (\"Tool\", \"USES_LANGUAGE\", \"ProgrammingLanguage\"),\n",
    "    (\"Framework\", \"USES_LANGUAGE\", \"ProgrammingLanguage\"),\n",
    "    (\"Concept\", \"IS_SUBTOPIC_OF\", \"Concept\")\n",
    "}\n",
    "# ==========================================\n",
    "# 3. HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "def clean_name(name):\n",
    "    if not name: return None\n",
    "    name = name.strip()\n",
    "    for pattern in ADJECTIVES:\n",
    "        name = re.sub(pattern, \"\", name, flags=re.IGNORECASE)\n",
    "    lower_name = name.lower()\n",
    "    if lower_name in CANONICAL_NAMES:\n",
    "        return CANONICAL_NAMES[lower_name]\n",
    "    if lower_name in [\"sql\", \"api\", \"etl\", \"elt\", \"bi\"]:\n",
    "        return name.upper()\n",
    "    return name.title()\n",
    "\n",
    "def validate_triplet(t):\n",
    "    # Flexible key access\n",
    "    s = t.get('subject') or t.get('subject_node')\n",
    "    o = t.get('object') or t.get('object_node')\n",
    "    r = t.get('relation') or t.get('relationship') or 'RELATED_TO'\n",
    "\n",
    "    s_clean = clean_name(s)\n",
    "    o_clean = clean_name(o)\n",
    "    if not s_clean or not o_clean: return None\n",
    "\n",
    "    s_type = TYPE_OVERRIDES.get(s_clean, t.get('subject_type', 'Concept'))\n",
    "    o_type = TYPE_OVERRIDES.get(o_clean, t.get('object_type', 'Concept'))\n",
    "\n",
    "    # Pattern Logic\n",
    "    if (s_type, r, o_type) not in ALLOWED_PATTERNS:\n",
    "        # Fallback for Wiki definitions\n",
    "        if r in [\"IMPLEMENTS_CONCEPT\", \"IS_SUBTOPIC_OF\"]:\n",
    "            pass\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    return {\n",
    "        \"subject\": s_clean, \"subject_type\": s_type,\n",
    "        \"relation\": r,\n",
    "        \"object\": o_clean, \"object_type\": o_type\n",
    "    }\n",
    "\n",
    "def chunker(seq, size):\n",
    "    seq_list = list(seq)\n",
    "    for i in range(0, len(seq_list), size):\n",
    "        yield seq_list[i:i + size]\n",
    "\n",
    "def generate_json_blocks(file_content):\n",
    "    \"\"\"\n",
    "    Generator that parses concatenated JSON objects from a string.\n",
    "    Works for pretty-printed JSONs stacked one after another.\n",
    "    \"\"\"\n",
    "    decoder = json.JSONDecoder()\n",
    "    pos = 0\n",
    "    while pos < len(file_content):\n",
    "        # Skip whitespace/newlines\n",
    "        while pos < len(file_content) and file_content[pos].isspace():\n",
    "            pos += 1\n",
    "        if pos >= len(file_content):\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            obj, end_pos = decoder.raw_decode(file_content, idx=pos)\n",
    "            yield obj\n",
    "            pos = end_pos\n",
    "        except json.JSONDecodeError:\n",
    "            # If we hit garbage, try to skip ahead or just break\n",
    "            # In a clean pipeline, we shouldn't hit this often if skipping whitespace worked\n",
    "            pos += 1\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN EXECUTION\n",
    "# ==========================================\n",
    "def main_validation_and_cypher():\n",
    "    print(f\"üöÄ Processing: {INPUT_FILE}\")\n",
    "\n",
    "    valid_triplets = []\n",
    "    seen_hashes = set()\n",
    "\n",
    "    # --- PHASE 1: STREAM PARSING & VALIDATION ---\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"‚ùå Error: {INPUT_FILE} not found.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "\n",
    "            # Use the STREAM PARSER here\n",
    "            block_count = 0\n",
    "            for data in generate_json_blocks(raw_text):\n",
    "                block_count += 1\n",
    "\n",
    "                # Normalize inputs (List vs Dict)\n",
    "                triplets_to_process = []\n",
    "                if isinstance(data, dict):\n",
    "                    triplets_to_process = data.get(\"triplets\", [])\n",
    "                elif isinstance(data, list):\n",
    "                    triplets_to_process = data\n",
    "\n",
    "                for raw in triplets_to_process:\n",
    "                    clean_t = validate_triplet(raw)\n",
    "                    if not clean_t: continue\n",
    "\n",
    "                    # Deduplicate\n",
    "                    h = f\"{clean_t['subject']}|{clean_t['relation']}|{clean_t['object']}\"\n",
    "                    if h not in seen_hashes:\n",
    "                        seen_hashes.add(h)\n",
    "                        valid_triplets.append(clean_t)\n",
    "\n",
    "                    # Inject Edges (Hydration)\n",
    "                    for side in ['subject', 'object']:\n",
    "                        token = clean_t[side]\n",
    "                        if token in INJECTED_EDGES:\n",
    "                            for rel, target, target_type in INJECTED_EDGES[token]:\n",
    "                                new_t = {\n",
    "                                    \"subject\": token,\n",
    "                                    \"subject_type\": clean_t[f'{side}_type'],\n",
    "                                    \"relation\": rel,\n",
    "                                    \"object\": target,\n",
    "                                    \"object_type\": target_type\n",
    "                                }\n",
    "                                h_new = f\"{new_t['subject']}|{new_t['relation']}|{new_t['object']}\"\n",
    "                                if h_new not in seen_hashes:\n",
    "                                    seen_hashes.add(h_new)\n",
    "                                    valid_triplets.append(new_t)\n",
    "\n",
    "            print(f\"üîç Successfully parsed {block_count} JSON blocks.\")\n",
    "\n",
    "        # Save Clean JSONL\n",
    "        with open(CLEAN_FILE, 'w', encoding='utf-8') as f_out:\n",
    "            for t in valid_triplets:\n",
    "                f_out.write(json.dumps(t) + \"\\n\")\n",
    "\n",
    "        print(f\"‚úÖ Cleaned {len(valid_triplets)} unique triplets.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical parsing error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- PHASE 2: CYPHER GENERATION ---\n",
    "    if not valid_triplets:\n",
    "        print(\"‚ö†Ô∏è No data to ingest.\")\n",
    "        return\n",
    "\n",
    "    print(\"‚öôÔ∏è Generating Cypher...\")\n",
    "    statements = [\"// INGESTION SCRIPT\\n\"]\n",
    "\n",
    "    # 1. Constraints\n",
    "    all_types = {t['subject_type'] for t in valid_triplets} | {t['object_type'] for t in valid_triplets}\n",
    "    for t in sorted(all_types):\n",
    "        statements.append(f\"CREATE CONSTRAINT {t.lower()}_uniq IF NOT EXISTS FOR (n:`{t}`) REQUIRE n.name IS UNIQUE;\")\n",
    "\n",
    "    # 2. Nodes\n",
    "    nodes = defaultdict(set)\n",
    "    for t in valid_triplets:\n",
    "        nodes[t['subject_type']].add(t['subject'])\n",
    "        nodes[t['object_type']].add(t['object'])\n",
    "\n",
    "    statements.append(\"\\n// --- NODES ---\")\n",
    "    for n_type, names in nodes.items():\n",
    "        for batch in chunker(names, BATCH_SIZE):\n",
    "            # Escape quotes for Cypher\n",
    "            safe_batch = [n.replace('\"', '\\\\\"') for n in batch]\n",
    "            statements.append(f\"UNWIND {json.dumps(safe_batch)} AS name MERGE (n:`{n_type}` {{name: name}});\")\n",
    "\n",
    "    # 3. Relationships\n",
    "    statements.append(\"\\n// --- RELATIONSHIPS ---\")\n",
    "    rels = defaultdict(list)\n",
    "    for t in valid_triplets:\n",
    "        key = (t['subject_type'], t['relation'], t['object_type'])\n",
    "        rels[key].append({\"s\": t['subject'], \"o\": t['object']})\n",
    "\n",
    "    for (st, rel, ot), pairs in rels.items():\n",
    "        for batch in chunker(pairs, BATCH_SIZE):\n",
    "            statements.append(\n",
    "                f\"UNWIND {json.dumps(batch)} AS row \"\n",
    "                f\"MATCH (s:`{st}` {{name: row.s}}) MATCH (o:`{ot}` {{name: row.o}}) \"\n",
    "                f\"MERGE (s)-[:{rel}]->(o);\"\n",
    "            )\n",
    "\n",
    "    with open(CYPHER_FILE, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\".join(statements))\n",
    "\n",
    "    print(f\"üéâ Success! Final script ready at: {CYPHER_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_validation_and_cypher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NE7gNlGmaqiD",
    "outputId": "17c54b2d-8b22-4365-e249-17da672c60bf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "BASE_DIR = \"/content/drive/MyDrive/rr_mlops/leftover_batches\"\n",
    "INPUT_FILE = os.path.join(BASE_DIR, \"clean_leftover_DAY6.jsonl\")\n",
    "OUTPUT_FILE = os.path.join(BASE_DIR, \"ingest_leftover_DAY6.cypher\")\n",
    "BATCH_SIZE = 1000  # Safe batch size for AuraDB\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER: CHUNKING\n",
    "# ==========================================\n",
    "def chunker(seq, size):\n",
    "    \"\"\"Yields chunks of a specific size from a list.\"\"\"\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield seq[i:i + size]\n",
    "\n",
    "# ==========================================\n",
    "# 3. GENERATORS\n",
    "# ==========================================\n",
    "def generate_constraints(types):\n",
    "    \"\"\"Generates unique constraints for every node type found.\"\"\"\n",
    "    return [\n",
    "        f\"CREATE CONSTRAINT {t.lower()}_uniq IF NOT EXISTS FOR (n:`{t}`) REQUIRE n.name IS UNIQUE;\"\n",
    "        for t in sorted(list(types))\n",
    "    ]\n",
    "\n",
    "def generate_node_cypher(triplets):\n",
    "    \"\"\"Generates batched UNWIND statements for Node creation.\"\"\"\n",
    "    nodes = {}\n",
    "\n",
    "    # 1. Aggregate unique names per type\n",
    "    for t in triplets:\n",
    "        for side in ['subject', 'object']:\n",
    "            typ = t[f'{side}_type']\n",
    "            name = t[f'{side}']\n",
    "            if typ not in nodes: nodes[typ] = set()\n",
    "            nodes[typ].add(name)\n",
    "\n",
    "    statements = []\n",
    "    statements.append(\"// --- 2. NODE CREATION ---\")\n",
    "\n",
    "    # 2. Generate Batched Queries\n",
    "    for n_type, names_set in nodes.items():\n",
    "        all_names = list(names_set)\n",
    "\n",
    "        for batch in chunker(all_names, BATCH_SIZE):\n",
    "            json_batch = json.dumps(batch)  # Safe JSON serializing\n",
    "\n",
    "            query = (\n",
    "                f\"// Batch: Create {n_type} nodes\\n\"\n",
    "                f\"UNWIND {json_batch} AS name \"\n",
    "                f\"MERGE (n:`{n_type}` {{name: name}});\"\n",
    "            )\n",
    "            statements.append(query)\n",
    "\n",
    "    return statements\n",
    "\n",
    "def generate_rel_cypher(triplets):\n",
    "    \"\"\"Generates batched UNWIND statements for Relationship creation.\"\"\"\n",
    "    rels = {}\n",
    "\n",
    "    # 1. Aggregate relations by signature\n",
    "    for t in triplets:\n",
    "        key = (t['subject_type'], t['relation'], t['object_type'])\n",
    "        if key not in rels: rels[key] = []\n",
    "\n",
    "        rels[key].append({\"s\": t['subject'], \"o\": t['object']})\n",
    "\n",
    "    statements = []\n",
    "    statements.append(\"// --- 3. RELATIONSHIP CREATION ---\")\n",
    "\n",
    "    # 2. Generate Batched Queries\n",
    "    for (st, rel, ot), pairs in rels.items():\n",
    "        for batch in chunker(pairs, BATCH_SIZE):\n",
    "            json_batch = json.dumps(batch)\n",
    "\n",
    "            query = (\n",
    "                f\"// Batch: {st} -> {rel} -> {ot}\\n\"\n",
    "                f\"UNWIND {json_batch} AS row \"\n",
    "                f\"MATCH (s:`{st}` {{name: row.s}}) \"\n",
    "                f\"MATCH (o:`{ot}` {{name: row.o}}) \"\n",
    "                f\"MERGE (s)-[:{rel}]->(o);\"\n",
    "            )\n",
    "            statements.append(query)\n",
    "\n",
    "    return statements\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    print(\"‚öôÔ∏è Generating Optimized Cypher Script...\")\n",
    "    triplets = []\n",
    "\n",
    "    try:\n",
    "        # Load JSONL\n",
    "        with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        triplets.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "        print(f\"üìä Loaded {len(triplets)} triplets.\")\n",
    "\n",
    "        if not triplets:\n",
    "            print(\"‚ö†Ô∏è Warning: No triplets loaded. Check your input file.\")\n",
    "            return\n",
    "\n",
    "        # 1. Generate Components\n",
    "        all_types = set()\n",
    "        for t in triplets:\n",
    "            all_types.add(t['subject_type'])\n",
    "            all_types.add(t['object_type'])\n",
    "\n",
    "        constraints = generate_constraints(all_types)\n",
    "        node_queries = generate_node_cypher(triplets)\n",
    "        rel_queries = generate_rel_cypher(triplets)\n",
    "\n",
    "        # 2. Write to File with Instructions\n",
    "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "            # Header Instructions\n",
    "            f.write(\"// ======================================================\\n\")\n",
    "            f.write(\"// INSTRUCTIONS FOR NEO4J BROWSER:\\n\")\n",
    "            f.write(\"// 1. Ensure 'Connect result nodes' is unchecked in settings (optional, for speed).\\n\")\n",
    "            f.write(\"// 2. Ensure 'Enable multi-statement query editor' is CHECKED.\\n\")\n",
    "            f.write(\"// 3. Copy-paste this entire file and run it.\\n\")\n",
    "            f.write(\"// ======================================================\\n\\n\")\n",
    "\n",
    "            f.write(\"// --- 1. CONSTRAINTS ---\\n\")\n",
    "            f.write(\"\\n\".join(constraints) + \"\\n\\n\")\n",
    "\n",
    "            # Double newline (\\n\\n) ensures clear separation for the Browser parser\n",
    "            f.write(\"\\n\\n\".join(node_queries) + \"\\n\\n\")\n",
    "            f.write(\"\\n\\n\".join(rel_queries) + \"\\n\")\n",
    "\n",
    "        print(f\"‚úÖ Success! Script saved to: {OUTPUT_FILE}\")\n",
    "        print(f\"   - Node Batches: {len(node_queries)}\")\n",
    "        print(f\"   - Relationship Batches: {len(rel_queries)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: Input file not found. Run validation script first.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpzM4gaGEe6Z"
   },
   "source": [
    "PUSHING ALL GENERATED CYPHERS TO DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdoEq-UCaqkc"
   },
   "outputs": [],
   "source": [
    "!pip install neo4j~=5.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzuCuwHsaqmw"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "NEO4J_PASSWORD= userdata.get('neo4j_rr_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwyACmsjaqo7",
    "outputId": "8e75d70f-7c08-4f98-f8a7-ba035bee896c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "from neo4j import GraphDatabase, exceptions\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "class Config:\n",
    "    # üéØ TARGET DIRECTORY: Where your clean leftover files live\n",
    "    SOURCE_DIR = \"/content/drive/MyDrive/rr_mlops/leftover_batches\"\n",
    "\n",
    "    # üîê DB CREDENTIALS\n",
    "    NEO4J_URI = \"neo4j+s://0c60999b.databases.neo4j.io\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD =  NEO4J_PASSWORD\n",
    "\n",
    "    BATCH_SIZE = 1000\n",
    "    MAX_RETRIES = 3\n",
    "\n",
    "    # Labels to enforce uniqueness on\n",
    "    ONTOLOGY_LABELS = [\n",
    "        \"JobPosting\", \"JobRole\", \"Skill\", \"Concept\",\n",
    "        \"ProgrammingLanguage\", \"Framework\", \"Tool\",\n",
    "        \"Platform\", \"CloudService\", \"Database\",\n",
    "        \"Company\", \"Location\"\n",
    "    ]\n",
    "\n",
    "# ==========================================\n",
    "# 2. ROBUST UTILS\n",
    "# ==========================================\n",
    "def run_with_retry(session, query, parameters, retries=Config.MAX_RETRIES):\n",
    "    \"\"\"Executes a query with transient error handling.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            result = session.run(query, parameters)\n",
    "            return result.consume()\n",
    "        except (exceptions.ServiceUnavailable, exceptions.TransientError) as e:\n",
    "            if attempt < retries - 1:\n",
    "                sleep_time = 2 * (attempt + 1)\n",
    "                print(f\"      ‚ö†Ô∏è Transient Error (Attempt {attempt+1}/{retries}). Retrying in {sleep_time}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "# ==========================================\n",
    "# 3. SCHEMA SAFETY (Constraints)\n",
    "# ==========================================\n",
    "def ensure_constraints(driver):\n",
    "    print(\"\\nüõ°Ô∏è Verifying Constraints (Speed & Integrity)...\")\n",
    "    with driver.session() as session:\n",
    "        for label in Config.ONTOLOGY_LABELS:\n",
    "            query = f\"CREATE CONSTRAINT {label.lower()}_uniq IF NOT EXISTS FOR (n:`{label}`) REQUIRE n.name IS UNIQUE\"\n",
    "            try:\n",
    "                session.run(query)\n",
    "                print(f\"   ‚úÖ Constraint active: :{label}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to set constraint for :{label} -> {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. DATA PREPARATION (Signature Grouping)\n",
    "# ==========================================\n",
    "def load_and_group_data(source_dir):\n",
    "    print(f\"\\nüìñ Scanning {source_dir} for cleaned data...\")\n",
    "\n",
    "    # We look for ANY .jsonl file, but prioritize 'clean' ones if they exist\n",
    "    files = sorted(glob.glob(os.path.join(source_dir, \"*.jsonl\")))\n",
    "\n",
    "    if not files:\n",
    "        print(\"‚ùå No .jsonl files found! Check your path.\")\n",
    "        return {}, {}\n",
    "\n",
    "    print(f\"   found {len(files)} files.\")\n",
    "\n",
    "    # 1. Group Nodes by Label: { \"JobRole\": {\"DevOps\", \"SRE\"}, ... }\n",
    "    nodes_by_label = {}\n",
    "\n",
    "    # 2. Group Edges by Signature: { (\"JobRole\", \"REQUIRES_SKILL\", \"Skill\"): [{\"s\": \"DevOps\", \"o\": \"Docker\"}, ...], ... }\n",
    "    edges_by_signature = {}\n",
    "\n",
    "    triplet_count = 0\n",
    "\n",
    "    for f_path in files:\n",
    "        print(f\"   ... processing {os.path.basename(f_path)}\")\n",
    "\n",
    "        with open(f_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if not line.strip(): continue\n",
    "                try:\n",
    "                    # Handle both single-line JSONL and potential block JSON\n",
    "                    data = json.loads(line)\n",
    "\n",
    "                    # Normalizer: sometimes data is a dict with \"triplets\", sometimes just the triplet dict\n",
    "                    if \"triplets\" in data:\n",
    "                        items = data[\"triplets\"]\n",
    "                    elif isinstance(data, list):\n",
    "                        items = data\n",
    "                    elif \"subject\" in data:\n",
    "                        items = [data] # It's a single triplet line\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    for t in items:\n",
    "                        s = t.get(\"subject\")\n",
    "                        st = t.get(\"subject_type\")\n",
    "                        r = t.get(\"relation\")\n",
    "                        o = t.get(\"object\")\n",
    "                        ot = t.get(\"object_type\")\n",
    "\n",
    "                        if all([s, st, r, o, ot]):\n",
    "                            # Add to Nodes (Set dedupes automatically)\n",
    "                            nodes_by_label.setdefault(st, set()).add(s)\n",
    "                            nodes_by_label.setdefault(ot, set()).add(o)\n",
    "\n",
    "                            # Add to Edges (Signature Based)\n",
    "                            sig = (st, r, ot)\n",
    "                            edges_by_signature.setdefault(sig, []).append({\"s\": s, \"o\": o})\n",
    "                            triplet_count += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "    print(f\"‚úÖ Parsed {triplet_count} valid triplets.\")\n",
    "    print(f\"   - {len(nodes_by_label)} Node Labels found\")\n",
    "    print(f\"   - {len(edges_by_signature)} Unique Relationship Patterns found\")\n",
    "\n",
    "    return nodes_by_label, edges_by_signature\n",
    "\n",
    "# ==========================================\n",
    "# 5. INGESTION ENGINE\n",
    "# ==========================================\n",
    "def ingest_data(driver, nodes_map, edges_map):\n",
    "    with driver.session() as session:\n",
    "\n",
    "        # --- PHASE 1: NODES (The Foundation) ---\n",
    "        print(\"\\nüèóÔ∏è Phase 1: Ingesting Nodes...\")\n",
    "        for label, names in nodes_map.items():\n",
    "            name_list = list(names)\n",
    "            total = len(name_list)\n",
    "            print(f\"   üîπ :{label} ({total} items)\")\n",
    "\n",
    "            for i in range(0, total, Config.BATCH_SIZE):\n",
    "                batch = name_list[i:i+Config.BATCH_SIZE]\n",
    "\n",
    "                # QUERY: Hardcoded Label -> Fast & Safe\n",
    "                query = f\"UNWIND $batch AS name MERGE (n:`{label}` {{name: name}})\"\n",
    "\n",
    "                try:\n",
    "                    run_with_retry(session, query, {\"batch\": batch})\n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚ùå Failed batch {i}: {e}\")\n",
    "\n",
    "        # --- PHASE 2: RELATIONSHIPS (The Context) ---\n",
    "        print(\"\\nüîó Phase 2: Ingesting Relationships...\")\n",
    "        for (s_label, rel_type, o_label), edge_list in edges_map.items():\n",
    "            total = len(edge_list)\n",
    "            print(f\"   üî∏ ({s_label}) -[:{rel_type}]-> ({o_label}) : {total} items\")\n",
    "\n",
    "            for i in range(0, total, Config.BATCH_SIZE):\n",
    "                batch = edge_list[i:i+Config.BATCH_SIZE]\n",
    "\n",
    "                # QUERY: Dynamic Construction, Static Execution\n",
    "                query = (\n",
    "                    f\"UNWIND $batch AS row \"\n",
    "                    f\"MATCH (s:`{s_label}` {{name: row.s}}) \"\n",
    "                    f\"MATCH (o:`{o_label}` {{name: row.o}}) \"\n",
    "                    f\"MERGE (s)-[:`{rel_type}`]->(o)\"\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    run_with_retry(session, query, {\"batch\": batch})\n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚ùå Failed batch {i}: {e}\")\n",
    "\n",
    "    print(\"\\nüéâ Ingestion Pipeline Complete!\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. MAIN\n",
    "# ==========================================\n",
    "def main():\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = GraphDatabase.driver(\n",
    "            Config.NEO4J_URI,\n",
    "            auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD)\n",
    "        )\n",
    "        driver.verify_connectivity()\n",
    "        print(\"üîå Connected to Neo4j AuraDB\")\n",
    "\n",
    "        # 1. Safety\n",
    "        ensure_constraints(driver)\n",
    "\n",
    "        # 2. Preparation\n",
    "        nodes, edges = load_and_group_data(Config.SOURCE_DIR)\n",
    "\n",
    "        if not nodes:\n",
    "            print(\"‚ö†Ô∏è No data found to ingest. Exiting.\")\n",
    "            return\n",
    "\n",
    "        # 3. Execution\n",
    "        ingest_data(driver, nodes, edges)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üíÄ Critical Error: {e}\")\n",
    "    finally:\n",
    "        if driver: driver.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_42seQ9aqrb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po6Q4wWRaquI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRxGIMG8aqwl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIlgEpcEaqzC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i130AVVAaq1r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmCYKnYdaq4S",
    "outputId": "2d5124d6-ab61-4bcf-b541-5701f6113bcb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (Match your Extraction Config)\n",
    "# ==========================================\n",
    "WIKI_PATH = \"/content/drive/MyDrive/rr_mlops/leftover_batches/wiki_source.jsonl\"\n",
    "BATCH_SIZE_CHARS = 53500\n",
    "\n",
    "def count_batches():\n",
    "    if not os.path.exists(WIKI_PATH):\n",
    "        print(f\"‚ùå File not found: {WIKI_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìä Analyzing: {os.path.basename(WIKI_PATH)}\")\n",
    "\n",
    "    total_chars = 0\n",
    "    total_items = 0\n",
    "    batch_count = 0\n",
    "    current_batch_len = 0\n",
    "\n",
    "    with open(WIKI_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "                # Mimic the Ingestor's capping logic (2000 words)\n",
    "                content = item.get('text', '')\n",
    "                words = content.split()\n",
    "                capped_content = \" \".join(words[:2500])\n",
    "\n",
    "                # Format the text exactly like it's sent to the LLM\n",
    "                chunk_text = f\"SOURCE: WIKI_ARTICLE\\nPRIMARY_ENTITY: {item['title']}\\nCONTENT: {capped_content}\\n\\n\"\n",
    "\n",
    "                chunk_len = len(chunk_text)\n",
    "                total_chars += chunk_len\n",
    "                total_items += 1\n",
    "\n",
    "                # Logic: If adding this chunk exceeds limit, it starts a new batch\n",
    "                if current_batch_len + chunk_len > BATCH_SIZE_CHARS:\n",
    "                    batch_count += 1\n",
    "                    current_batch_len = chunk_len\n",
    "                else:\n",
    "                    current_batch_len += chunk_len\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    # Don't forget the last remaining batch\n",
    "    if current_batch_len > 0:\n",
    "        batch_count += 1\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"üìù Total Articles:    {total_items}\")\n",
    "    print(f\"üî† Total Characters:  {total_chars:,}\")\n",
    "    print(f\"üì¶ Total LLM Batches: {batch_count}\")\n",
    "    print(f\"‚è±Ô∏è Est. Time:        ~{batch_count * 15 / 60:.1f} minutes (at 15s/batch)\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    count_batches()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
